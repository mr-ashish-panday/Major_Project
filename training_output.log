max_steps is given, it will override any value given in num_train_epochs
✓ Loaded cached Common Voice 16.0 dataset:
DatasetDict({
    train: Dataset({
        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],
        num_rows: 283
    })
    validation: Dataset({
        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],
        num_rows: 205
    })
    test: Dataset({
        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],
        num_rows: 193
    })
})
✓ Dataset after removing columns:
DatasetDict({
    train: Dataset({
        features: ['audio', 'sentence'],
        num_rows: 283
    })
    validation: Dataset({
        features: ['audio', 'sentence'],
        num_rows: 205
    })
    test: Dataset({
        features: ['audio', 'sentence'],
        num_rows: 193
    })
})
✓ First training example:
{'audio': {'path': '/home/ashish/common_voice_cache/commonvoice/downloads/extracted/7ca4b2599aef730d54648dcc42b7f7e6d44d7bec27b3eecb5bc8a745a8c93714/ne-NP_train_0/common_voice_ne-NP_35314089.mp3', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,
        7.00167766e-06, -4.02070254e-05, -3.65305859e-05]), 'sampling_rate': 48000}, 'sentence': 'म पनि जान्छु है त अहिले लाई ।'}
✓ First training example after downsampling:
{'audio': {'path': '/home/ashish/common_voice_cache/commonvoice/downloads/extracted/7ca4b2599aef730d54648dcc42b7f7e6d44d7bec27b3eecb5bc8a745a8c93714/ne-NP_train_0/common_voice_ne-NP_35314089.mp3', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,
        9.42416955e-06,  5.37711458e-06, -5.44977956e-06]), 'sampling_rate': 16000}, 'sentence': 'म पनि जान्छु है त अहिले लाई ।'}
  0%|          | 0/1000 [00:00<?, ?it/s]/home/ashish/common_voice_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...
  0%|          | 1/1000 [00:07<2:10:29,  7.84s/it]  0%|          | 2/1000 [00:12<1:39:50,  6.00s/it]  0%|          | 3/1000 [00:17<1:29:27,  5.38s/it]  0%|          | 4/1000 [00:22<1:28:30,  5.33s/it]  0%|          | 5/1000 [00:28<1:33:12,  5.62s/it]  1%|          | 6/1000 [00:33<1:30:39,  5.47s/it]  1%|          | 7/1000 [00:39<1:29:46,  5.42s/it]  1%|          | 8/1000 [00:44<1:27:49,  5.31s/it]  1%|          | 9/1000 [00:48<1:25:01,  5.15s/it]  1%|          | 10/1000 [00:53<1:21:49,  4.96s/it]Step 10/1000 (1.0%) | Loss: 1.6849                                                     1%|          | 10/1000 [00:53<1:21:49,  4.96s/it]  1%|          | 11/1000 [00:58<1:20:15,  4.87s/it]  1%|          | 12/1000 [01:03<1:20:06,  4.86s/it]  1%|▏         | 13/1000 [01:07<1:20:18,  4.88s/it]  1%|▏         | 14/1000 [01:12<1:20:52,  4.92s/it]  2%|▏         | 15/1000 [01:17<1:20:06,  4.88s/it]  2%|▏         | 16/1000 [01:22<1:18:23,  4.78s/it]  2%|▏         | 17/1000 [01:27<1:18:03,  4.76s/it]  2%|▏         | 18/1000 [01:29<1:04:57,  3.97s/it]  2%|▏         | 19/1000 [01:35<1:16:30,  4.68s/it]  2%|▏         | 20/1000 [01:40<1:17:05,  4.72s/it]{'loss': 1.6849, 'grad_norm': 19.487472534179688, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.56}
Step 20/1000 (2.0%) | Loss: 1.4629                                                     2%|▏         | 20/1000 [01:40<1:17:05,  4.72s/it]  2%|▏         | 21/1000 [01:45<1:17:21,  4.74s/it]  2%|▏         | 22/1000 [01:49<1:16:42,  4.71s/it]  2%|▏         | 23/1000 [01:54<1:15:53,  4.66s/it]  2%|▏         | 24/1000 [01:58<1:15:28,  4.64s/it]  2%|▎         | 25/1000 [02:03<1:17:07,  4.75s/it]  3%|▎         | 26/1000 [02:08<1:17:19,  4.76s/it]  3%|▎         | 27/1000 [02:13<1:19:43,  4.92s/it]  3%|▎         | 28/1000 [02:18<1:18:39,  4.86s/it]  3%|▎         | 29/1000 [02:23<1:17:25,  4.78s/it]  3%|▎         | 30/1000 [02:27<1:16:52,  4.76s/it]{'loss': 1.4629, 'grad_norm': 17.7143497467041, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.11}
Step 30/1000 (3.0%) | Loss: 1.2346                                                     3%|▎         | 30/1000 [02:27<1:16:52,  4.76s/it]  3%|▎         | 31/1000 [02:32<1:16:34,  4.74s/it]  3%|▎         | 32/1000 [02:39<1:24:39,  5.25s/it]  3%|▎         | 33/1000 [02:43<1:21:23,  5.05s/it]  3%|▎         | 34/1000 [02:48<1:19:17,  4.92s/it]  4%|▎         | 35/1000 [02:52<1:17:52,  4.84s/it]  4%|▎         | 36/1000 [02:54<1:04:00,  3.98s/it]  4%|▎         | 37/1000 [03:00<1:12:43,  4.53s/it]  4%|▍         | 38/1000 [03:05<1:13:05,  4.56s/it]  4%|▍         | 39/1000 [03:10<1:13:38,  4.60s/it]  4%|▍         | 40/1000 [03:14<1:13:31,  4.60s/it]{'loss': 1.2346, 'grad_norm': 14.928144454956055, 'learning_rate': 1.5e-06, 'epoch': 1.67}
Step 40/1000 (4.0%) | Loss: 1.0212                                                     4%|▍         | 40/1000 [03:14<1:13:31,  4.60s/it]  4%|▍         | 41/1000 [03:19<1:13:25,  4.59s/it]  4%|▍         | 42/1000 [03:24<1:15:44,  4.74s/it]  4%|▍         | 43/1000 [03:29<1:15:32,  4.74s/it]  4%|▍         | 44/1000 [03:33<1:15:18,  4.73s/it]  4%|▍         | 45/1000 [03:38<1:14:46,  4.70s/it]  5%|▍         | 46/1000 [03:43<1:14:30,  4.69s/it]  5%|▍         | 47/1000 [03:47<1:14:32,  4.69s/it]  5%|▍         | 48/1000 [03:52<1:13:45,  4.65s/it]  5%|▍         | 49/1000 [03:57<1:14:07,  4.68s/it]  5%|▌         | 50/1000 [04:01<1:13:17,  4.63s/it]{'loss': 1.0212, 'grad_norm': 11.038225173950195, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.22}
Step 50/1000 (5.0%) | Loss: 0.8220                                                     5%|▌         | 50/1000 [04:01<1:13:17,  4.63s/it]  5%|▌         | 51/1000 [04:06<1:13:08,  4.62s/it]  5%|▌         | 52/1000 [04:10<1:12:58,  4.62s/it]  5%|▌         | 53/1000 [04:15<1:13:22,  4.65s/it]  5%|▌         | 54/1000 [04:17<1:00:31,  3.84s/it]  6%|▌         | 55/1000 [04:23<1:12:08,  4.58s/it]  6%|▌         | 56/1000 [04:29<1:19:53,  5.08s/it]  6%|▌         | 57/1000 [04:34<1:17:34,  4.94s/it]  6%|▌         | 58/1000 [04:39<1:15:48,  4.83s/it]  6%|▌         | 59/1000 [04:43<1:15:03,  4.79s/it]  6%|▌         | 60/1000 [04:49<1:17:15,  4.93s/it]{'loss': 0.822, 'grad_norm': 8.506583213806152, 'learning_rate': 2.5e-06, 'epoch': 2.78}
Step 60/1000 (6.0%) | Loss: 0.7231                                                     6%|▌         | 60/1000 [04:49<1:17:15,  4.93s/it]  6%|▌         | 61/1000 [04:53<1:15:13,  4.81s/it]  6%|▌         | 62/1000 [04:58<1:14:42,  4.78s/it]  6%|▋         | 63/1000 [05:02<1:13:59,  4.74s/it]  6%|▋         | 64/1000 [05:07<1:13:13,  4.69s/it]  6%|▋         | 65/1000 [05:12<1:13:04,  4.69s/it]  7%|▋         | 66/1000 [05:17<1:13:45,  4.74s/it]  7%|▋         | 67/1000 [05:21<1:13:32,  4.73s/it]  7%|▋         | 68/1000 [05:26<1:12:41,  4.68s/it]  7%|▋         | 69/1000 [05:30<1:12:02,  4.64s/it]  7%|▋         | 70/1000 [05:35<1:11:47,  4.63s/it]{'loss': 0.7231, 'grad_norm': 9.052428245544434, 'learning_rate': 3e-06, 'epoch': 3.33}
Step 70/1000 (7.0%) | Loss: 0.5766                                                     7%|▋         | 70/1000 [05:35<1:11:47,  4.63s/it]  7%|▋         | 71/1000 [05:42<1:20:44,  5.21s/it]  7%|▋         | 72/1000 [05:44<1:05:39,  4.24s/it]  7%|▋         | 73/1000 [05:50<1:13:19,  4.75s/it]  7%|▋         | 74/1000 [05:57<1:24:07,  5.45s/it]  8%|▊         | 75/1000 [06:02<1:22:07,  5.33s/it]  8%|▊         | 76/1000 [06:06<1:19:08,  5.14s/it]  8%|▊         | 77/1000 [06:12<1:20:30,  5.23s/it]  8%|▊         | 78/1000 [06:17<1:18:30,  5.11s/it]  8%|▊         | 79/1000 [06:22<1:17:43,  5.06s/it]  8%|▊         | 80/1000 [06:27<1:20:47,  5.27s/it]{'loss': 0.5766, 'grad_norm': 7.421472072601318, 'learning_rate': 3.5e-06, 'epoch': 3.89}
Step 80/1000 (8.0%) | Loss: 0.4814                                                     8%|▊         | 80/1000 [06:27<1:20:47,  5.27s/it]  8%|▊         | 81/1000 [06:32<1:18:08,  5.10s/it]  8%|▊         | 82/1000 [06:37<1:15:42,  4.95s/it]  8%|▊         | 83/1000 [06:42<1:19:28,  5.20s/it]  8%|▊         | 84/1000 [06:47<1:17:40,  5.09s/it]  8%|▊         | 85/1000 [06:52<1:15:31,  4.95s/it]  9%|▊         | 86/1000 [06:59<1:26:49,  5.70s/it]  9%|▊         | 87/1000 [07:06<1:33:25,  6.14s/it]  9%|▉         | 88/1000 [07:12<1:28:12,  5.80s/it]  9%|▉         | 89/1000 [07:16<1:22:59,  5.47s/it]  9%|▉         | 90/1000 [07:18<1:07:00,  4.42s/it]{'loss': 0.4814, 'grad_norm': 6.580539226531982, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.44}
Step 90/1000 (9.0%) | Loss: 0.4260                                                     9%|▉         | 90/1000 [07:18<1:07:00,  4.42s/it]  9%|▉         | 91/1000 [07:24<1:13:14,  4.83s/it]  9%|▉         | 92/1000 [07:29<1:13:51,  4.88s/it]  9%|▉         | 93/1000 [07:34<1:12:32,  4.80s/it]  9%|▉         | 94/1000 [07:38<1:11:33,  4.74s/it] 10%|▉         | 95/1000 [07:43<1:10:19,  4.66s/it] 10%|▉         | 96/1000 [07:47<1:09:49,  4.63s/it] 10%|▉         | 97/1000 [07:52<1:12:27,  4.81s/it] 10%|▉         | 98/1000 [07:57<1:11:47,  4.78s/it] 10%|▉         | 99/1000 [08:02<1:11:46,  4.78s/it] 10%|█         | 100/1000 [08:07<1:13:10,  4.88s/it]{'loss': 0.426, 'grad_norm': 7.534013748168945, 'learning_rate': 4.5e-06, 'epoch': 5.0}
Step 100/1000 (10.0%) | Loss: 0.3278                                                     10%|█         | 100/1000 [08:07<1:13:10,  4.88s/it] 10%|█         | 101/1000 [08:12<1:12:34,  4.84s/it] 10%|█         | 102/1000 [08:16<1:10:52,  4.74s/it] 10%|█         | 103/1000 [08:21<1:10:50,  4.74s/it] 10%|█         | 104/1000 [08:26<1:10:23,  4.71s/it] 10%|█         | 105/1000 [08:30<1:10:05,  4.70s/it] 11%|█         | 106/1000 [08:36<1:15:22,  5.06s/it] 11%|█         | 107/1000 [08:42<1:20:13,  5.39s/it] 11%|█         | 108/1000 [08:45<1:07:20,  4.53s/it] 11%|█         | 109/1000 [08:52<1:16:44,  5.17s/it] 11%|█         | 110/1000 [08:56<1:14:01,  4.99s/it]{'loss': 0.3278, 'grad_norm': 5.409830093383789, 'learning_rate': 5e-06, 'epoch': 5.56}
Step 110/1000 (11.0%) | Loss: 0.2942                                                     11%|█         | 110/1000 [08:56<1:14:01,  4.99s/it] 11%|█         | 111/1000 [09:01<1:13:39,  4.97s/it] 11%|█         | 112/1000 [09:07<1:18:12,  5.28s/it] 11%|█▏        | 113/1000 [09:14<1:27:28,  5.92s/it] 11%|█▏        | 114/1000 [09:19<1:21:03,  5.49s/it] 12%|█▏        | 115/1000 [09:23<1:15:59,  5.15s/it] 12%|█▏        | 116/1000 [09:28<1:13:00,  4.96s/it] 12%|█▏        | 117/1000 [09:32<1:11:25,  4.85s/it] 12%|█▏        | 118/1000 [09:37<1:10:19,  4.78s/it] 12%|█▏        | 119/1000 [09:42<1:10:08,  4.78s/it] 12%|█▏        | 120/1000 [09:49<1:20:13,  5.47s/it]{'loss': 0.2942, 'grad_norm': 5.135933876037598, 'learning_rate': 5.500000000000001e-06, 'epoch': 6.11}
Step 120/1000 (12.0%) | Loss: 0.2041                                                     12%|█▏        | 120/1000 [09:49<1:20:13,  5.47s/it] 12%|█▏        | 121/1000 [09:54<1:16:27,  5.22s/it] 12%|█▏        | 122/1000 [09:58<1:13:41,  5.04s/it] 12%|█▏        | 123/1000 [10:03<1:13:49,  5.05s/it] 12%|█▏        | 124/1000 [10:08<1:13:05,  5.01s/it] 12%|█▎        | 125/1000 [10:14<1:18:07,  5.36s/it] 13%|█▎        | 126/1000 [10:16<1:03:34,  4.36s/it] 13%|█▎        | 127/1000 [10:23<1:13:00,  5.02s/it] 13%|█▎        | 128/1000 [10:28<1:12:13,  4.97s/it] 13%|█▎        | 129/1000 [10:33<1:11:43,  4.94s/it] 13%|█▎        | 130/1000 [10:37<1:09:30,  4.79s/it]{'loss': 0.2041, 'grad_norm': 5.4811601638793945, 'learning_rate': 6e-06, 'epoch': 6.67}
Step 130/1000 (13.0%) | Loss: 0.1778                                                     13%|█▎        | 130/1000 [10:37<1:09:30,  4.79s/it] 13%|█▎        | 131/1000 [10:42<1:08:16,  4.71s/it] 13%|█▎        | 132/1000 [10:46<1:07:12,  4.65s/it] 13%|█▎        | 133/1000 [10:51<1:09:56,  4.84s/it] 13%|█▎        | 134/1000 [10:58<1:16:46,  5.32s/it] 14%|█▎        | 135/1000 [11:02<1:13:12,  5.08s/it] 14%|█▎        | 136/1000 [11:07<1:11:31,  4.97s/it] 14%|█▎        | 137/1000 [11:12<1:10:04,  4.87s/it] 14%|█▍        | 138/1000 [11:17<1:09:43,  4.85s/it] 14%|█▍        | 139/1000 [11:21<1:09:04,  4.81s/it] 14%|█▍        | 140/1000 [11:26<1:07:27,  4.71s/it]{'loss': 0.1778, 'grad_norm': 3.34169602394104, 'learning_rate': 6.5000000000000004e-06, 'epoch': 7.22}
Step 140/1000 (14.0%) | Loss: 0.1143                                                     14%|█▍        | 140/1000 [11:26<1:07:27,  4.71s/it] 14%|█▍        | 141/1000 [11:30<1:06:58,  4.68s/it] 14%|█▍        | 142/1000 [11:35<1:07:01,  4.69s/it] 14%|█▍        | 143/1000 [11:40<1:06:59,  4.69s/it] 14%|█▍        | 144/1000 [11:42<55:11,  3.87s/it]   14%|█▍        | 145/1000 [11:47<1:02:28,  4.38s/it] 15%|█▍        | 146/1000 [11:53<1:07:19,  4.73s/it] 15%|█▍        | 147/1000 [11:58<1:07:24,  4.74s/it] 15%|█▍        | 148/1000 [12:02<1:06:04,  4.65s/it] 15%|█▍        | 149/1000 [12:07<1:05:54,  4.65s/it] 15%|█▌        | 150/1000 [12:11<1:05:11,  4.60s/it]{'loss': 0.1143, 'grad_norm': 4.41959810256958, 'learning_rate': 7e-06, 'epoch': 7.78}
Step 150/1000 (15.0%) | Loss: 0.0933                                                     15%|█▌        | 150/1000 [12:11<1:05:11,  4.60s/it] 15%|█▌        | 151/1000 [12:16<1:05:11,  4.61s/it] 15%|█▌        | 152/1000 [12:20<1:04:50,  4.59s/it] 15%|█▌        | 153/1000 [12:25<1:06:30,  4.71s/it] 15%|█▌        | 154/1000 [12:30<1:08:09,  4.83s/it] 16%|█▌        | 155/1000 [12:35<1:07:49,  4.82s/it] 16%|█▌        | 156/1000 [12:40<1:08:37,  4.88s/it] 16%|█▌        | 157/1000 [12:46<1:11:29,  5.09s/it] 16%|█▌        | 158/1000 [12:51<1:11:48,  5.12s/it] 16%|█▌        | 159/1000 [12:56<1:10:43,  5.05s/it] 16%|█▌        | 160/1000 [13:01<1:09:04,  4.93s/it]{'loss': 0.0933, 'grad_norm': 2.413787364959717, 'learning_rate': 7.500000000000001e-06, 'epoch': 8.33}
Step 160/1000 (16.0%) | Loss: 0.0634                                                     16%|█▌        | 160/1000 [13:01<1:09:04,  4.93s/it] 16%|█▌        | 161/1000 [13:05<1:07:40,  4.84s/it] 16%|█▌        | 162/1000 [13:07<55:28,  3.97s/it]   16%|█▋        | 163/1000 [13:14<1:06:27,  4.76s/it] 16%|█▋        | 164/1000 [13:19<1:08:14,  4.90s/it] 16%|█▋        | 165/1000 [13:24<1:06:56,  4.81s/it] 17%|█▋        | 166/1000 [13:28<1:07:10,  4.83s/it] 17%|█▋        | 167/1000 [13:34<1:08:09,  4.91s/it] 17%|█▋        | 168/1000 [13:38<1:06:07,  4.77s/it] 17%|█▋        | 169/1000 [13:42<1:04:45,  4.68s/it] 17%|█▋        | 170/1000 [13:47<1:04:11,  4.64s/it]{'loss': 0.0634, 'grad_norm': 5.4282050132751465, 'learning_rate': 8.000000000000001e-06, 'epoch': 8.89}
Step 170/1000 (17.0%) | Loss: 0.0434                                                     17%|█▋        | 170/1000 [13:47<1:04:11,  4.64s/it] 17%|█▋        | 171/1000 [13:52<1:07:19,  4.87s/it] 17%|█▋        | 172/1000 [13:59<1:13:51,  5.35s/it] 17%|█▋        | 173/1000 [14:03<1:09:49,  5.07s/it] 17%|█▋        | 174/1000 [14:08<1:06:55,  4.86s/it] 18%|█▊        | 175/1000 [14:12<1:05:08,  4.74s/it] 18%|█▊        | 176/1000 [14:17<1:04:55,  4.73s/it] 18%|█▊        | 177/1000 [14:22<1:04:56,  4.73s/it] 18%|█▊        | 178/1000 [14:26<1:03:59,  4.67s/it] 18%|█▊        | 179/1000 [14:31<1:03:38,  4.65s/it] 18%|█▊        | 180/1000 [14:33<52:48,  3.86s/it]  {'loss': 0.0434, 'grad_norm': 2.38938570022583, 'learning_rate': 8.5e-06, 'epoch': 9.44}
Step 180/1000 (18.0%) | Loss: 0.0373                                                   18%|█▊        | 180/1000 [14:33<52:48,  3.86s/it] 18%|█▊        | 181/1000 [14:39<1:00:50,  4.46s/it] 18%|█▊        | 182/1000 [14:44<1:02:59,  4.62s/it] 18%|█▊        | 183/1000 [14:48<1:02:40,  4.60s/it] 18%|█▊        | 184/1000 [14:53<1:04:20,  4.73s/it] 18%|█▊        | 185/1000 [14:58<1:03:28,  4.67s/it] 19%|█▊        | 186/1000 [15:03<1:06:09,  4.88s/it] 19%|█▊        | 187/1000 [15:08<1:04:55,  4.79s/it] 19%|█▉        | 188/1000 [15:12<1:04:23,  4.76s/it] 19%|█▉        | 189/1000 [15:17<1:06:01,  4.88s/it] 19%|█▉        | 190/1000 [15:23<1:06:54,  4.96s/it]{'loss': 0.0373, 'grad_norm': 4.551729202270508, 'learning_rate': 9e-06, 'epoch': 10.0}
Step 190/1000 (19.0%) | Loss: 0.0324                                                     19%|█▉        | 190/1000 [15:23<1:06:54,  4.96s/it] 19%|█▉        | 191/1000 [15:27<1:06:03,  4.90s/it] 19%|█▉        | 192/1000 [15:33<1:07:21,  5.00s/it] 19%|█▉        | 193/1000 [15:38<1:07:00,  4.98s/it] 19%|█▉        | 194/1000 [15:42<1:05:29,  4.87s/it] 20%|█▉        | 195/1000 [15:47<1:04:33,  4.81s/it] 20%|█▉        | 196/1000 [15:53<1:08:27,  5.11s/it] 20%|█▉        | 197/1000 [15:58<1:07:25,  5.04s/it] 20%|█▉        | 198/1000 [15:59<55:07,  4.12s/it]   20%|█▉        | 199/1000 [16:05<1:01:53,  4.64s/it] 20%|██        | 200/1000 [16:10<1:02:24,  4.68s/it]{'loss': 0.0324, 'grad_norm': 4.463846206665039, 'learning_rate': 9.5e-06, 'epoch': 10.56}
Step 200/1000 (20.0%) | Loss: 0.0330                                                     20%|██        | 200/1000 [16:10<1:02:24,  4.68s/it] 20%|██        | 201/1000 [16:15<1:02:13,  4.67s/it] 20%|██        | 202/1000 [16:20<1:06:13,  4.98s/it] 20%|██        | 203/1000 [16:26<1:09:15,  5.21s/it] 20%|██        | 204/1000 [16:31<1:06:43,  5.03s/it] 20%|██        | 205/1000 [16:35<1:04:37,  4.88s/it] 21%|██        | 206/1000 [16:40<1:04:11,  4.85s/it] 21%|██        | 207/1000 [16:45<1:03:21,  4.79s/it] 21%|██        | 208/1000 [16:49<1:02:11,  4.71s/it] 21%|██        | 209/1000 [16:54<1:01:30,  4.67s/it] 21%|██        | 210/1000 [16:59<1:01:25,  4.67s/it]{'loss': 0.033, 'grad_norm': 2.987075090408325, 'learning_rate': 1e-05, 'epoch': 11.11}
Step 210/1000 (21.0%) | Loss: 0.0275                                                     21%|██        | 210/1000 [16:59<1:01:25,  4.67s/it] 21%|██        | 211/1000 [17:03<1:01:15,  4.66s/it] 21%|██        | 212/1000 [17:08<1:01:45,  4.70s/it] 21%|██▏       | 213/1000 [17:13<1:01:59,  4.73s/it] 21%|██▏       | 214/1000 [17:17<1:00:46,  4.64s/it] 22%|██▏       | 215/1000 [17:22<1:00:30,  4.63s/it] 22%|██▏       | 216/1000 [17:24<50:01,  3.83s/it]   22%|██▏       | 217/1000 [17:30<58:21,  4.47s/it] 22%|██▏       | 218/1000 [17:35<59:37,  4.57s/it] 22%|██▏       | 219/1000 [17:39<59:28,  4.57s/it] 22%|██▏       | 220/1000 [17:46<1:08:23,  5.26s/it]{'loss': 0.0275, 'grad_norm': 3.9656567573547363, 'learning_rate': 9.875000000000001e-06, 'epoch': 11.67}
Step 220/1000 (22.0%) | Loss: 0.0241                                                     22%|██▏       | 220/1000 [17:46<1:08:23,  5.26s/it] 22%|██▏       | 221/1000 [17:51<1:07:19,  5.19s/it] 22%|██▏       | 222/1000 [17:57<1:08:54,  5.31s/it] 22%|██▏       | 223/1000 [18:02<1:08:26,  5.29s/it] 22%|██▏       | 224/1000 [18:07<1:06:15,  5.12s/it] 22%|██▎       | 225/1000 [18:11<1:03:35,  4.92s/it] 23%|██▎       | 226/1000 [18:16<1:02:39,  4.86s/it] 23%|██▎       | 227/1000 [18:21<1:02:52,  4.88s/it] 23%|██▎       | 228/1000 [18:27<1:08:50,  5.35s/it] 23%|██▎       | 229/1000 [18:32<1:05:18,  5.08s/it] 23%|██▎       | 230/1000 [18:37<1:04:50,  5.05s/it]{'loss': 0.0241, 'grad_norm': 2.5688633918762207, 'learning_rate': 9.75e-06, 'epoch': 12.22}
Step 230/1000 (23.0%) | Loss: 0.0251                                                     23%|██▎       | 230/1000 [18:37<1:04:50,  5.05s/it] 23%|██▎       | 231/1000 [18:41<1:03:20,  4.94s/it] 23%|██▎       | 232/1000 [18:46<1:03:58,  5.00s/it] 23%|██▎       | 233/1000 [18:52<1:05:46,  5.15s/it] 23%|██▎       | 234/1000 [18:54<53:39,  4.20s/it]   24%|██▎       | 235/1000 [19:00<59:28,  4.67s/it] 24%|██▎       | 236/1000 [19:06<1:06:55,  5.26s/it] 24%|██▎       | 237/1000 [19:11<1:04:47,  5.10s/it] 24%|██▍       | 238/1000 [19:16<1:02:53,  4.95s/it] 24%|██▍       | 239/1000 [19:20<1:01:24,  4.84s/it] 24%|██▍       | 240/1000 [19:25<1:01:07,  4.83s/it]{'loss': 0.0251, 'grad_norm': 8.201539993286133, 'learning_rate': 9.625e-06, 'epoch': 12.78}
Step 240/1000 (24.0%) | Loss: 0.0207                                                     24%|██▍       | 240/1000 [19:25<1:01:07,  4.83s/it] 24%|██▍       | 241/1000 [19:30<1:02:35,  4.95s/it] 24%|██▍       | 242/1000 [19:35<1:02:25,  4.94s/it] 24%|██▍       | 243/1000 [19:40<1:00:44,  4.81s/it] 24%|██▍       | 244/1000 [19:44<59:27,  4.72s/it]   24%|██▍       | 245/1000 [19:49<59:44,  4.75s/it] 25%|██▍       | 246/1000 [19:53<58:42,  4.67s/it] 25%|██▍       | 247/1000 [19:58<58:33,  4.67s/it] 25%|██▍       | 248/1000 [20:03<57:54,  4.62s/it] 25%|██▍       | 249/1000 [20:07<58:44,  4.69s/it] 25%|██▌       | 250/1000 [20:12<58:24,  4.67s/it]{'loss': 0.0207, 'grad_norm': 7.691979885101318, 'learning_rate': 9.5e-06, 'epoch': 13.33}
Step 250/1000 (25.0%) | Loss: 0.0195                                                   25%|██▌       | 250/1000 [20:12<58:24,  4.67s/it]You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
{'loss': 0.0195, 'grad_norm': 3.980623483657837, 'learning_rate': 9.375000000000001e-06, 'epoch': 13.89}

  0%|          | 0/49 [00:00<?, ?it/s][A
  4%|▍         | 2/49 [00:02<00:47,  1.01s/it][A
  6%|▌         | 3/49 [00:03<00:58,  1.28s/it][A
  8%|▊         | 4/49 [00:05<01:01,  1.37s/it][A
 10%|█         | 5/49 [00:06<00:59,  1.35s/it][A
 12%|█▏        | 6/49 [00:08<01:05,  1.53s/it][A
 14%|█▍        | 7/49 [00:10<01:07,  1.60s/it][A
 16%|█▋        | 8/49 [00:12<01:16,  1.87s/it][A
 18%|█▊        | 9/49 [00:14<01:15,  1.88s/it][A
 20%|██        | 10/49 [00:16<01:13,  1.88s/it][A
 22%|██▏       | 11/49 [00:18<01:10,  1.87s/it][A
 24%|██▍       | 12/49 [00:19<01:07,  1.83s/it][A
 27%|██▋       | 13/49 [00:22<01:12,  2.01s/it][A
 29%|██▊       | 14/49 [00:24<01:08,  1.95s/it][A
 31%|███       | 15/49 [00:25<01:03,  1.87s/it][A
 33%|███▎      | 16/49 [00:27<00:59,  1.79s/it][A
 35%|███▍      | 17/49 [00:29<00:57,  1.80s/it][A
 37%|███▋      | 18/49 [00:31<00:56,  1.83s/it][A
 39%|███▉      | 19/49 [00:32<00:53,  1.78s/it][A
 41%|████      | 20/49 [00:35<00:54,  1.89s/it][A
 43%|████▎     | 21/49 [00:37<00:59,  2.13s/it][A
 45%|████▍     | 22/49 [00:39<00:57,  2.13s/it][A
 47%|████▋     | 23/49 [00:41<00:51,  1.98s/it][A
 49%|████▉     | 24/49 [00:43<00:46,  1.86s/it][A
 51%|█████     | 25/49 [00:45<00:45,  1.88s/it][A
 53%|█████▎    | 26/49 [00:46<00:42,  1.84s/it][A
 55%|█████▌    | 27/49 [00:48<00:39,  1.81s/it][A
 57%|█████▋    | 28/49 [00:50<00:39,  1.86s/it][A
 59%|█████▉    | 29/49 [00:52<00:36,  1.84s/it][A
 61%|██████    | 30/49 [00:55<00:40,  2.16s/it][A
 63%|██████▎   | 31/49 [00:57<00:38,  2.16s/it][A
 65%|██████▌   | 32/49 [00:58<00:33,  1.97s/it][A
 67%|██████▋   | 33/49 [01:00<00:29,  1.81s/it][A
 69%|██████▉   | 34/49 [01:03<00:32,  2.15s/it][A
 71%|███████▏  | 35/49 [01:04<00:28,  2.00s/it][A
 73%|███████▎  | 36/49 [01:06<00:22,  1.76s/it][A
 76%|███████▌  | 37/49 [01:07<00:19,  1.61s/it][A
 78%|███████▊  | 38/49 [01:09<00:18,  1.65s/it][A
 80%|███████▉  | 39/49 [01:11<00:18,  1.84s/it][A
 82%|████████▏ | 40/49 [01:13<00:16,  1.81s/it][A
 84%|████████▎ | 41/49 [01:14<00:13,  1.74s/it][A
 86%|████████▌ | 42/49 [01:16<00:11,  1.69s/it][A
 88%|████████▊ | 43/49 [01:17<00:09,  1.63s/it][A
 90%|████████▉ | 44/49 [01:19<00:08,  1.69s/it][A
 92%|█████████▏| 45/49 [01:20<00:06,  1.57s/it][A
 94%|█████████▍| 46/49 [01:22<00:04,  1.50s/it][A
 96%|█████████▌| 47/49 [01:23<00:02,  1.47s/it][A
 98%|█████████▊| 48/49 [01:24<00:01,  1.34s/it][A
100%|██████████| 49/49 [01:25<00:00,  1.06s/it][A                                                  
                                               [A 25%|██▌       | 250/1000 [21:58<58:24,  4.67s/it]
100%|██████████| 49/49 [01:42<00:00,  1.06s/it][A
                                               [ASome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}
/home/ashish/common_voice_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 25%|██▌       | 251/1000 [22:21<8:42:50, 41.88s/it] 25%|██▌       | 252/1000 [22:23<6:14:40, 30.05s/it] 25%|██▌       | 253/1000 [22:31<4:51:54, 23.45s/it] 25%|██▌       | 254/1000 [22:38<3:50:59, 18.58s/it] 26%|██▌       | 255/1000 [22:43<2:58:31, 14.38s/it] 26%|██▌       | 256/1000 [22:48<2:21:46, 11.43s/it] 26%|██▌       | 257/1000 [22:52<1:56:24,  9.40s/it] 26%|██▌       | 258/1000 [22:57<1:40:08,  8.10s/it] 26%|██▌       | 259/1000 [23:02<1:26:58,  7.04s/it] 26%|██▌       | 260/1000 [23:07<1:18:16,  6.35s/it]{'eval_loss': 0.7368482351303101, 'eval_wer': 71.89490445859873, 'eval_runtime': 105.6978, 'eval_samples_per_second': 1.826, 'eval_steps_per_second': 0.464, 'epoch': 13.89}
Step 260/1000 (26.0%) | Loss: 0.0125                                                     26%|██▌       | 260/1000 [23:07<1:18:16,  6.35s/it] 26%|██▌       | 261/1000 [23:13<1:17:56,  6.33s/it] 26%|██▌       | 262/1000 [23:18<1:12:28,  5.89s/it] 26%|██▋       | 263/1000 [23:23<1:10:53,  5.77s/it] 26%|██▋       | 264/1000 [23:28<1:06:23,  5.41s/it] 26%|██▋       | 265/1000 [23:33<1:03:59,  5.22s/it] 27%|██▋       | 266/1000 [23:38<1:03:27,  5.19s/it] 27%|██▋       | 267/1000 [23:43<1:03:20,  5.18s/it] 27%|██▋       | 268/1000 [23:48<1:03:33,  5.21s/it] 27%|██▋       | 269/1000 [23:53<1:03:31,  5.21s/it] 27%|██▋       | 270/1000 [23:55<51:53,  4.27s/it]  {'loss': 0.0125, 'grad_norm': 2.909569501876831, 'learning_rate': 9.250000000000001e-06, 'epoch': 14.44}
Step 270/1000 (27.0%) | Loss: 0.0179                                                   27%|██▋       | 270/1000 [23:55<51:53,  4.27s/it] 27%|██▋       | 271/1000 [24:01<57:55,  4.77s/it] 27%|██▋       | 272/1000 [24:06<57:03,  4.70s/it] 27%|██▋       | 273/1000 [24:10<56:11,  4.64s/it] 27%|██▋       | 274/1000 [24:15<56:35,  4.68s/it] 28%|██▊       | 275/1000 [24:20<56:30,  4.68s/it] 28%|██▊       | 276/1000 [24:25<56:14,  4.66s/it] 28%|██▊       | 277/1000 [24:29<57:07,  4.74s/it] 28%|██▊       | 278/1000 [24:34<56:42,  4.71s/it] 28%|██▊       | 279/1000 [24:39<56:17,  4.68s/it] 28%|██▊       | 280/1000 [24:44<57:02,  4.75s/it]{'loss': 0.0179, 'grad_norm': 4.382036209106445, 'learning_rate': 9.125e-06, 'epoch': 15.0}
Step 280/1000 (28.0%) | Loss: 0.0102                                                   28%|██▊       | 280/1000 [24:44<57:02,  4.75s/it] 28%|██▊       | 281/1000 [24:48<57:21,  4.79s/it] 28%|██▊       | 282/1000 [24:53<56:53,  4.75s/it] 28%|██▊       | 283/1000 [24:58<57:45,  4.83s/it] 28%|██▊       | 284/1000 [25:03<57:22,  4.81s/it] 28%|██▊       | 285/1000 [25:08<59:25,  4.99s/it] 29%|██▊       | 286/1000 [25:14<1:00:17,  5.07s/it] 29%|██▊       | 287/1000 [25:19<1:01:25,  5.17s/it] 29%|██▉       | 288/1000 [25:22<52:03,  4.39s/it]   29%|██▉       | 289/1000 [25:28<57:31,  4.85s/it] 29%|██▉       | 290/1000 [25:34<1:02:14,  5.26s/it]{'loss': 0.0102, 'grad_norm': 1.4156571626663208, 'learning_rate': 9e-06, 'epoch': 15.56}
Step 290/1000 (29.0%) | Loss: 0.0146                                                     29%|██▉       | 290/1000 [25:34<1:02:14,  5.26s/it] 29%|██▉       | 291/1000 [25:40<1:04:51,  5.49s/it] 29%|██▉       | 292/1000 [25:45<1:02:50,  5.33s/it] 29%|██▉       | 293/1000 [25:50<1:02:07,  5.27s/it] 29%|██▉       | 294/1000 [25:55<59:56,  5.09s/it]   30%|██▉       | 295/1000 [25:59<58:15,  4.96s/it] 30%|██▉       | 296/1000 [26:04<58:50,  5.01s/it] 30%|██▉       | 297/1000 [26:09<58:05,  4.96s/it] 30%|██▉       | 298/1000 [26:14<57:09,  4.89s/it] 30%|██▉       | 299/1000 [26:20<59:55,  5.13s/it] 30%|███       | 300/1000 [26:24<58:06,  4.98s/it]{'loss': 0.0146, 'grad_norm': 0.5503398776054382, 'learning_rate': 8.875e-06, 'epoch': 16.11}
Step 300/1000 (30.0%) | Loss: 0.0125                                                   30%|███       | 300/1000 [26:24<58:06,  4.98s/it] 30%|███       | 301/1000 [26:29<58:17,  5.00s/it] 30%|███       | 302/1000 [26:35<59:24,  5.11s/it] 30%|███       | 303/1000 [26:39<58:12,  5.01s/it] 30%|███       | 304/1000 [26:46<1:02:22,  5.38s/it] 30%|███       | 305/1000 [26:50<1:00:36,  5.23s/it] 31%|███       | 306/1000 [26:53<51:00,  4.41s/it]   31%|███       | 307/1000 [26:59<57:45,  5.00s/it] 31%|███       | 308/1000 [27:04<56:34,  4.91s/it] 31%|███       | 309/1000 [27:09<56:08,  4.88s/it] 31%|███       | 310/1000 [27:14<57:47,  5.03s/it]{'loss': 0.0125, 'grad_norm': 2.3762168884277344, 'learning_rate': 8.750000000000001e-06, 'epoch': 16.67}
Step 310/1000 (31.0%) | Loss: 0.0145                                                   31%|███       | 310/1000 [27:14<57:47,  5.03s/it] 31%|███       | 311/1000 [27:21<1:02:14,  5.42s/it] 31%|███       | 312/1000 [27:25<59:28,  5.19s/it]   31%|███▏      | 313/1000 [27:30<57:34,  5.03s/it] 31%|███▏      | 314/1000 [27:34<55:57,  4.89s/it] 32%|███▏      | 315/1000 [27:39<54:57,  4.81s/it] 32%|███▏      | 316/1000 [27:44<55:30,  4.87s/it] 32%|███▏      | 317/1000 [27:49<55:41,  4.89s/it] 32%|███▏      | 318/1000 [27:54<54:12,  4.77s/it] 32%|███▏      | 319/1000 [27:58<53:55,  4.75s/it] 32%|███▏      | 320/1000 [28:03<54:11,  4.78s/it]{'loss': 0.0145, 'grad_norm': 0.5339083671569824, 'learning_rate': 8.625000000000001e-06, 'epoch': 17.22}
Step 320/1000 (32.0%) | Loss: 0.0077                                                   32%|███▏      | 320/1000 [28:03<54:11,  4.78s/it] 32%|███▏      | 321/1000 [28:08<55:03,  4.87s/it] 32%|███▏      | 322/1000 [28:13<53:52,  4.77s/it] 32%|███▏      | 323/1000 [28:17<53:17,  4.72s/it] 32%|███▏      | 324/1000 [28:19<43:51,  3.89s/it] 32%|███▎      | 325/1000 [28:25<49:51,  4.43s/it] 33%|███▎      | 326/1000 [28:30<50:22,  4.48s/it] 33%|███▎      | 327/1000 [28:34<51:12,  4.56s/it] 33%|███▎      | 328/1000 [28:40<53:27,  4.77s/it] 33%|███▎      | 329/1000 [28:44<53:29,  4.78s/it] 33%|███▎      | 330/1000 [28:50<57:32,  5.15s/it]{'loss': 0.0077, 'grad_norm': 2.7170426845550537, 'learning_rate': 8.5e-06, 'epoch': 17.78}
Step 330/1000 (33.0%) | Loss: 0.0074                                                   33%|███▎      | 330/1000 [28:50<57:32,  5.15s/it] 33%|███▎      | 331/1000 [28:55<56:37,  5.08s/it] 33%|███▎      | 332/1000 [29:00<54:43,  4.92s/it] 33%|███▎      | 333/1000 [29:04<53:51,  4.84s/it] 33%|███▎      | 334/1000 [29:09<53:22,  4.81s/it] 34%|███▎      | 335/1000 [29:15<56:27,  5.09s/it] 34%|███▎      | 336/1000 [29:20<54:37,  4.94s/it] 34%|███▎      | 337/1000 [29:24<53:38,  4.85s/it] 34%|███▍      | 338/1000 [29:29<52:25,  4.75s/it] 34%|███▍      | 339/1000 [29:34<52:42,  4.78s/it] 34%|███▍      | 340/1000 [29:40<58:05,  5.28s/it]{'loss': 0.0074, 'grad_norm': 2.945669174194336, 'learning_rate': 8.375e-06, 'epoch': 18.33}
Step 340/1000 (34.0%) | Loss: 0.0076                                                   34%|███▍      | 340/1000 [29:40<58:05,  5.28s/it] 34%|███▍      | 341/1000 [29:45<56:00,  5.10s/it] 34%|███▍      | 342/1000 [29:47<47:33,  4.34s/it] 34%|███▍      | 343/1000 [29:54<56:38,  5.17s/it] 34%|███▍      | 344/1000 [29:59<55:18,  5.06s/it] 34%|███▍      | 345/1000 [30:05<58:58,  5.40s/it] 35%|███▍      | 346/1000 [30:10<56:28,  5.18s/it] 35%|███▍      | 347/1000 [30:15<55:52,  5.13s/it] 35%|███▍      | 348/1000 [30:20<53:41,  4.94s/it] 35%|███▍      | 349/1000 [30:24<52:46,  4.86s/it] 35%|███▌      | 350/1000 [30:29<52:09,  4.81s/it]{'loss': 0.0076, 'grad_norm': 2.5424294471740723, 'learning_rate': 8.25e-06, 'epoch': 18.89}
Step 350/1000 (35.0%) | Loss: 0.0074                                                   35%|███▌      | 350/1000 [30:29<52:09,  4.81s/it] 35%|███▌      | 351/1000 [30:35<55:50,  5.16s/it] 35%|███▌      | 352/1000 [30:40<54:18,  5.03s/it] 35%|███▌      | 353/1000 [30:44<53:16,  4.94s/it] 35%|███▌      | 354/1000 [30:49<52:48,  4.90s/it] 36%|███▌      | 355/1000 [30:55<54:39,  5.09s/it] 36%|███▌      | 356/1000 [30:59<52:52,  4.93s/it] 36%|███▌      | 357/1000 [31:04<52:17,  4.88s/it] 36%|███▌      | 358/1000 [31:09<51:37,  4.82s/it] 36%|███▌      | 359/1000 [31:14<51:39,  4.84s/it] 36%|███▌      | 360/1000 [31:16<42:25,  3.98s/it]{'loss': 0.0074, 'grad_norm': 0.24102725088596344, 'learning_rate': 8.125000000000001e-06, 'epoch': 19.44}
Step 360/1000 (36.0%) | Loss: 0.0051                                                   36%|███▌      | 360/1000 [31:16<42:25,  3.98s/it] 36%|███▌      | 361/1000 [31:22<49:07,  4.61s/it] 36%|███▌      | 362/1000 [31:26<49:09,  4.62s/it] 36%|███▋      | 363/1000 [31:31<49:06,  4.63s/it] 36%|███▋      | 364/1000 [31:37<52:19,  4.94s/it] 36%|███▋      | 365/1000 [31:42<52:36,  4.97s/it] 37%|███▋      | 366/1000 [31:46<51:36,  4.88s/it] 37%|███▋      | 367/1000 [31:51<51:15,  4.86s/it] 37%|███▋      | 368/1000 [31:56<50:35,  4.80s/it] 37%|███▋      | 369/1000 [32:01<51:24,  4.89s/it] 37%|███▋      | 370/1000 [32:06<51:04,  4.86s/it]{'loss': 0.0051, 'grad_norm': 0.2023524045944214, 'learning_rate': 8.000000000000001e-06, 'epoch': 20.0}
Step 370/1000 (37.0%) | Loss: 0.0036                                                   37%|███▋      | 370/1000 [32:06<51:04,  4.86s/it] 37%|███▋      | 371/1000 [32:10<49:53,  4.76s/it] 37%|███▋      | 372/1000 [32:15<50:13,  4.80s/it] 37%|███▋      | 373/1000 [32:20<50:32,  4.84s/it] 37%|███▋      | 374/1000 [32:25<51:20,  4.92s/it] 38%|███▊      | 375/1000 [32:30<50:05,  4.81s/it] 38%|███▊      | 376/1000 [32:34<49:20,  4.74s/it] 38%|███▊      | 377/1000 [32:39<48:39,  4.69s/it] 38%|███▊      | 378/1000 [32:41<40:12,  3.88s/it] 38%|███▊      | 379/1000 [32:47<46:28,  4.49s/it] 38%|███▊      | 380/1000 [32:51<46:27,  4.50s/it]{'loss': 0.0036, 'grad_norm': 0.42988595366477966, 'learning_rate': 7.875e-06, 'epoch': 20.56}
Step 380/1000 (38.0%) | Loss: 0.0035                                                   38%|███▊      | 380/1000 [32:51<46:27,  4.50s/it] 38%|███▊      | 381/1000 [32:56<46:22,  4.50s/it] 38%|███▊      | 382/1000 [33:01<48:17,  4.69s/it] 38%|███▊      | 383/1000 [33:06<49:48,  4.84s/it] 38%|███▊      | 384/1000 [33:13<55:33,  5.41s/it] 38%|███▊      | 385/1000 [33:18<54:04,  5.27s/it] 39%|███▊      | 386/1000 [33:23<52:30,  5.13s/it] 39%|███▊      | 387/1000 [33:27<50:46,  4.97s/it] 39%|███▉      | 388/1000 [33:32<49:39,  4.87s/it] 39%|███▉      | 389/1000 [33:36<48:43,  4.78s/it] 39%|███▉      | 390/1000 [33:41<48:29,  4.77s/it]{'loss': 0.0035, 'grad_norm': 0.7986001372337341, 'learning_rate': 7.75e-06, 'epoch': 21.11}
Step 390/1000 (39.0%) | Loss: 0.0020                                                   39%|███▉      | 390/1000 [33:41<48:29,  4.77s/it] 39%|███▉      | 391/1000 [33:47<51:10,  5.04s/it] 39%|███▉      | 392/1000 [33:52<52:22,  5.17s/it] 39%|███▉      | 393/1000 [33:57<51:04,  5.05s/it] 39%|███▉      | 394/1000 [34:02<50:02,  4.95s/it] 40%|███▉      | 395/1000 [34:07<52:11,  5.18s/it] 40%|███▉      | 396/1000 [34:10<42:42,  4.24s/it] 40%|███▉      | 397/1000 [34:15<47:45,  4.75s/it] 40%|███▉      | 398/1000 [34:20<47:37,  4.75s/it] 40%|███▉      | 399/1000 [34:27<52:55,  5.28s/it] 40%|████      | 400/1000 [34:32<51:58,  5.20s/it]{'loss': 0.002, 'grad_norm': 0.09715001285076141, 'learning_rate': 7.625e-06, 'epoch': 21.67}
Step 400/1000 (40.0%) | Loss: 0.0035                                                   40%|████      | 400/1000 [34:32<51:58,  5.20s/it] 40%|████      | 401/1000 [34:36<49:48,  4.99s/it] 40%|████      | 402/1000 [34:41<49:47,  5.00s/it] 40%|████      | 403/1000 [34:46<49:35,  4.98s/it] 40%|████      | 404/1000 [34:53<53:30,  5.39s/it] 40%|████      | 405/1000 [34:57<50:58,  5.14s/it] 41%|████      | 406/1000 [35:02<49:00,  4.95s/it] 41%|████      | 407/1000 [35:06<47:35,  4.82s/it] 41%|████      | 408/1000 [35:11<48:25,  4.91s/it] 41%|████      | 409/1000 [35:16<47:48,  4.85s/it] 41%|████      | 410/1000 [35:21<46:53,  4.77s/it]{'loss': 0.0035, 'grad_norm': 0.35052523016929626, 'learning_rate': 7.500000000000001e-06, 'epoch': 22.22}
Step 410/1000 (41.0%) | Loss: 0.0031                                                   41%|████      | 410/1000 [35:21<46:53,  4.77s/it] 41%|████      | 411/1000 [35:26<48:25,  4.93s/it] 41%|████      | 412/1000 [35:30<47:13,  4.82s/it] 41%|████▏     | 413/1000 [35:35<47:01,  4.81s/it] 41%|████▏     | 414/1000 [35:37<38:37,  3.95s/it] 42%|████▏     | 415/1000 [35:43<44:08,  4.53s/it] 42%|████▏     | 416/1000 [35:49<49:03,  5.04s/it] 42%|████▏     | 417/1000 [35:54<47:41,  4.91s/it] 42%|████▏     | 418/1000 [35:59<47:56,  4.94s/it] 42%|████▏     | 419/1000 [36:03<46:29,  4.80s/it] 42%|████▏     | 420/1000 [36:08<45:32,  4.71s/it]{'loss': 0.0031, 'grad_norm': 0.2538907825946808, 'learning_rate': 7.375000000000001e-06, 'epoch': 22.78}
Step 420/1000 (42.0%) | Loss: 0.0011                                                   42%|████▏     | 420/1000 [36:08<45:32,  4.71s/it] 42%|████▏     | 421/1000 [36:13<46:21,  4.80s/it] 42%|████▏     | 422/1000 [36:17<45:30,  4.72s/it] 42%|████▏     | 423/1000 [36:23<48:40,  5.06s/it] 42%|████▏     | 424/1000 [36:28<47:11,  4.92s/it] 42%|████▎     | 425/1000 [36:33<49:18,  5.15s/it] 43%|████▎     | 426/1000 [36:40<53:34,  5.60s/it] 43%|████▎     | 427/1000 [36:45<51:16,  5.37s/it] 43%|████▎     | 428/1000 [36:52<54:31,  5.72s/it] 43%|████▎     | 429/1000 [36:56<51:32,  5.42s/it] 43%|████▎     | 430/1000 [37:01<49:25,  5.20s/it]{'loss': 0.0011, 'grad_norm': 0.09749853610992432, 'learning_rate': 7.25e-06, 'epoch': 23.33}
Step 430/1000 (43.0%) | Loss: 0.0016                                                   43%|████▎     | 430/1000 [37:01<49:25,  5.20s/it] 43%|████▎     | 431/1000 [37:06<47:36,  5.02s/it] 43%|████▎     | 432/1000 [37:08<38:58,  4.12s/it] 43%|████▎     | 433/1000 [37:14<44:27,  4.71s/it] 43%|████▎     | 434/1000 [37:21<52:15,  5.54s/it] 44%|████▎     | 435/1000 [37:26<50:04,  5.32s/it] 44%|████▎     | 436/1000 [37:30<47:56,  5.10s/it] 44%|████▎     | 437/1000 [37:35<46:10,  4.92s/it] 44%|████▍     | 438/1000 [37:40<46:19,  4.95s/it] 44%|████▍     | 439/1000 [37:45<45:56,  4.91s/it] 44%|████▍     | 440/1000 [37:50<47:00,  5.04s/it]{'loss': 0.0016, 'grad_norm': 0.05182107537984848, 'learning_rate': 7.125e-06, 'epoch': 23.89}
Step 440/1000 (44.0%) | Loss: 0.0018                                                   44%|████▍     | 440/1000 [37:50<47:00,  5.04s/it] 44%|████▍     | 441/1000 [37:55<46:54,  5.04s/it] 44%|████▍     | 442/1000 [38:00<46:48,  5.03s/it] 44%|████▍     | 443/1000 [38:05<46:28,  5.01s/it] 44%|████▍     | 444/1000 [38:10<46:47,  5.05s/it] 44%|████▍     | 445/1000 [38:15<45:59,  4.97s/it] 45%|████▍     | 446/1000 [38:20<44:46,  4.85s/it] 45%|████▍     | 447/1000 [38:24<44:28,  4.83s/it] 45%|████▍     | 448/1000 [38:29<43:50,  4.76s/it] 45%|████▍     | 449/1000 [38:34<43:31,  4.74s/it] 45%|████▌     | 450/1000 [38:36<35:46,  3.90s/it]{'loss': 0.0018, 'grad_norm': 7.050579071044922, 'learning_rate': 7e-06, 'epoch': 24.44}
Step 450/1000 (45.0%) | Loss: 0.0011                                                   45%|████▌     | 450/1000 [38:36<35:46,  3.90s/it] 45%|████▌     | 451/1000 [38:42<42:33,  4.65s/it] 45%|████▌     | 452/1000 [38:47<42:27,  4.65s/it] 45%|████▌     | 453/1000 [38:52<42:55,  4.71s/it] 45%|████▌     | 454/1000 [38:57<45:23,  4.99s/it] 46%|████▌     | 455/1000 [39:02<44:06,  4.86s/it] 46%|████▌     | 456/1000 [39:06<43:15,  4.77s/it] 46%|████▌     | 457/1000 [39:11<43:20,  4.79s/it] 46%|████▌     | 458/1000 [39:16<43:37,  4.83s/it] 46%|████▌     | 459/1000 [39:21<43:01,  4.77s/it] 46%|████▌     | 460/1000 [39:26<43:05,  4.79s/it]{'loss': 0.0011, 'grad_norm': 0.07067766785621643, 'learning_rate': 6.875e-06, 'epoch': 25.0}
Step 460/1000 (46.0%) | Loss: 0.0008                                                   46%|████▌     | 460/1000 [39:26<43:05,  4.79s/it] 46%|████▌     | 461/1000 [39:30<42:35,  4.74s/it] 46%|████▌     | 462/1000 [39:36<44:09,  4.93s/it] 46%|████▋     | 463/1000 [39:40<43:32,  4.87s/it] 46%|████▋     | 464/1000 [39:45<43:00,  4.81s/it] 46%|████▋     | 465/1000 [39:50<43:03,  4.83s/it] 47%|████▋     | 466/1000 [39:55<43:11,  4.85s/it] 47%|████▋     | 467/1000 [40:00<43:21,  4.88s/it] 47%|████▋     | 468/1000 [40:02<37:06,  4.19s/it] 47%|████▋     | 469/1000 [40:08<42:24,  4.79s/it] 47%|████▋     | 470/1000 [40:15<46:56,  5.31s/it]{'loss': 0.0008, 'grad_norm': 0.042480625212192535, 'learning_rate': 6.750000000000001e-06, 'epoch': 25.56}
Step 470/1000 (47.0%) | Loss: 0.0006                                                   47%|████▋     | 470/1000 [40:15<46:56,  5.31s/it] 47%|████▋     | 471/1000 [40:20<45:25,  5.15s/it] 47%|████▋     | 472/1000 [40:25<45:14,  5.14s/it] 47%|████▋     | 473/1000 [40:30<44:03,  5.02s/it] 47%|████▋     | 474/1000 [40:34<42:56,  4.90s/it] 48%|████▊     | 475/1000 [40:40<44:30,  5.09s/it] 48%|████▊     | 476/1000 [40:46<47:44,  5.47s/it] 48%|████▊     | 477/1000 [40:51<45:42,  5.24s/it] 48%|████▊     | 478/1000 [40:56<44:41,  5.14s/it] 48%|████▊     | 479/1000 [41:01<44:34,  5.13s/it] 48%|████▊     | 480/1000 [41:07<46:03,  5.31s/it]{'loss': 0.0006, 'grad_norm': 0.028995295986533165, 'learning_rate': 6.625e-06, 'epoch': 26.11}
Step 480/1000 (48.0%) | Loss: 0.0006                                                   48%|████▊     | 480/1000 [41:07<46:03,  5.31s/it] 48%|████▊     | 481/1000 [41:11<44:32,  5.15s/it] 48%|████▊     | 482/1000 [41:16<43:05,  4.99s/it] 48%|████▊     | 483/1000 [41:21<41:59,  4.87s/it] 48%|████▊     | 484/1000 [41:26<42:14,  4.91s/it] 48%|████▊     | 485/1000 [41:32<47:00,  5.48s/it] 49%|████▊     | 486/1000 [41:34<38:09,  4.46s/it] 49%|████▊     | 487/1000 [41:41<42:20,  4.95s/it] 49%|████▉     | 488/1000 [41:45<41:43,  4.89s/it] 49%|████▉     | 489/1000 [41:50<41:46,  4.91s/it] 49%|████▉     | 490/1000 [41:55<41:58,  4.94s/it]{'loss': 0.0006, 'grad_norm': 0.05523747578263283, 'learning_rate': 6.5000000000000004e-06, 'epoch': 26.67}
Step 490/1000 (49.0%) | Loss: 0.0005                                                   49%|████▉     | 490/1000 [41:55<41:58,  4.94s/it] 49%|████▉     | 491/1000 [42:01<43:22,  5.11s/it] 49%|████▉     | 492/1000 [42:05<42:09,  4.98s/it] 49%|████▉     | 493/1000 [42:10<41:40,  4.93s/it] 49%|████▉     | 494/1000 [42:15<41:10,  4.88s/it] 50%|████▉     | 495/1000 [42:20<42:11,  5.01s/it] 50%|████▉     | 496/1000 [42:25<41:24,  4.93s/it] 50%|████▉     | 497/1000 [42:30<42:21,  5.05s/it] 50%|████▉     | 498/1000 [42:36<42:25,  5.07s/it] 50%|████▉     | 499/1000 [42:42<46:21,  5.55s/it] 50%|█████     | 500/1000 [42:47<44:44,  5.37s/it]{'loss': 0.0005, 'grad_norm': 0.025215912610292435, 'learning_rate': 6.375e-06, 'epoch': 27.22}
Step 500/1000 (50.0%) | Loss: 0.0005                                                   50%|█████     | 500/1000 [42:47<44:44,  5.37s/it]{'loss': 0.0005, 'grad_norm': 0.019441023468971252, 'learning_rate': 6.25e-06, 'epoch': 27.78}

  0%|          | 0/49 [00:00<?, ?it/s][A
  4%|▍         | 2/49 [00:01<00:38,  1.21it/s][A
  6%|▌         | 3/49 [00:03<00:48,  1.05s/it][A
  8%|▊         | 4/49 [00:04<00:53,  1.19s/it][A
 10%|█         | 5/49 [00:05<00:54,  1.24s/it][A
 12%|█▏        | 6/49 [00:07<01:05,  1.52s/it][A
 14%|█▍        | 7/49 [00:09<01:07,  1.61s/it][A
 16%|█▋        | 8/49 [00:11<01:03,  1.56s/it][A
 18%|█▊        | 9/49 [00:13<01:11,  1.78s/it][A
 20%|██        | 10/49 [00:14<01:07,  1.73s/it][A
 22%|██▏       | 11/49 [00:16<01:04,  1.69s/it][A
 24%|██▍       | 12/49 [00:17<00:59,  1.61s/it][A
 27%|██▋       | 13/49 [00:19<00:59,  1.64s/it][A
 29%|██▊       | 14/49 [00:21<00:56,  1.60s/it][A
 31%|███       | 15/49 [00:22<00:54,  1.61s/it][A
 33%|███▎      | 16/49 [00:24<00:54,  1.66s/it][A
 35%|███▍      | 17/49 [00:26<00:55,  1.73s/it][A
 37%|███▋      | 18/49 [00:28<00:55,  1.79s/it][A
 39%|███▉      | 19/49 [00:29<00:49,  1.66s/it][A
 41%|████      | 20/49 [00:31<00:52,  1.80s/it][A
 43%|████▎     | 21/49 [00:34<00:55,  2.00s/it][A
 45%|████▍     | 22/49 [00:36<00:54,  2.03s/it][A
 47%|████▋     | 23/49 [00:38<00:50,  1.93s/it][A
 49%|████▉     | 24/49 [00:39<00:45,  1.84s/it][A
 51%|█████     | 25/49 [00:41<00:41,  1.75s/it][A
 53%|█████▎    | 26/49 [00:42<00:36,  1.60s/it][A
 55%|█████▌    | 27/49 [00:44<00:34,  1.55s/it][A
 57%|█████▋    | 28/49 [00:45<00:31,  1.52s/it][A
 59%|█████▉    | 29/49 [00:46<00:28,  1.43s/it][A
 61%|██████    | 30/49 [00:48<00:28,  1.53s/it][A
 63%|██████▎   | 31/49 [00:50<00:29,  1.66s/it][A
 65%|██████▌   | 32/49 [00:51<00:27,  1.60s/it][A
 67%|██████▋   | 33/49 [00:53<00:24,  1.51s/it][A
 69%|██████▉   | 34/49 [00:54<00:23,  1.57s/it][A
 71%|███████▏  | 35/49 [00:56<00:22,  1.59s/it][A
 73%|███████▎  | 36/49 [00:57<00:19,  1.48s/it][A
 76%|███████▌  | 37/49 [00:59<00:17,  1.48s/it][A
 78%|███████▊  | 38/49 [01:01<00:17,  1.58s/it][A
 80%|███████▉  | 39/49 [01:02<00:16,  1.68s/it][A
 82%|████████▏ | 40/49 [01:04<00:15,  1.73s/it][A
 84%|████████▎ | 41/49 [01:06<00:14,  1.80s/it][A
 86%|████████▌ | 42/49 [01:08<00:12,  1.73s/it][A
 88%|████████▊ | 43/49 [01:09<00:10,  1.68s/it][A
 90%|████████▉ | 44/49 [01:11<00:08,  1.75s/it][A
 92%|█████████▏| 45/49 [01:13<00:06,  1.65s/it][A
 94%|█████████▍| 46/49 [01:14<00:04,  1.57s/it][A
 96%|█████████▌| 47/49 [01:16<00:03,  1.61s/it][A
 98%|█████████▊| 48/49 [01:17<00:01,  1.49s/it][A
100%|██████████| 49/49 [01:17<00:00,  1.15s/it][A                                                  
                                               [A 50%|█████     | 500/1000 [44:25<44:44,  5.37s/it]
100%|██████████| 49/49 [01:35<00:00,  1.15s/it][A
                                               [ASome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}
/home/ashish/common_voice_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 50%|█████     | 501/1000 [44:35<5:00:34, 36.14s/it] 50%|█████     | 502/1000 [44:41<3:44:17, 27.02s/it] 50%|█████     | 503/1000 [44:46<2:48:44, 20.37s/it] 50%|█████     | 504/1000 [44:48<2:02:52, 14.86s/it] 50%|█████     | 505/1000 [44:54<1:40:36, 12.19s/it] 51%|█████     | 506/1000 [44:59<1:22:23, 10.01s/it] 51%|█████     | 507/1000 [45:04<1:10:13,  8.55s/it] 51%|█████     | 508/1000 [45:10<1:05:46,  8.02s/it] 51%|█████     | 509/1000 [45:15<57:03,  6.97s/it]   51%|█████     | 510/1000 [45:20<51:35,  6.32s/it]{'eval_loss': 0.8461319208145142, 'eval_wer': 69.90445859872611, 'eval_runtime': 97.9524, 'eval_samples_per_second': 1.97, 'eval_steps_per_second': 0.5, 'epoch': 27.78}
Step 510/1000 (51.0%) | Loss: 0.0005                                                   51%|█████     | 510/1000 [45:20<51:35,  6.32s/it] 51%|█████     | 511/1000 [45:25<48:01,  5.89s/it] 51%|█████     | 512/1000 [45:29<44:52,  5.52s/it] 51%|█████▏    | 513/1000 [45:34<42:20,  5.22s/it] 51%|█████▏    | 514/1000 [45:39<42:37,  5.26s/it] 52%|█████▏    | 515/1000 [45:45<43:35,  5.39s/it] 52%|█████▏    | 516/1000 [45:50<41:42,  5.17s/it] 52%|█████▏    | 517/1000 [45:54<39:59,  4.97s/it] 52%|█████▏    | 518/1000 [45:59<39:26,  4.91s/it] 52%|█████▏    | 519/1000 [46:04<39:03,  4.87s/it] 52%|█████▏    | 520/1000 [46:09<39:49,  4.98s/it]{'loss': 0.0005, 'grad_norm': 0.019566064700484276, 'learning_rate': 6.125000000000001e-06, 'epoch': 28.33}
Step 520/1000 (52.0%) | Loss: 0.0004                                                   52%|█████▏    | 520/1000 [46:09<39:49,  4.98s/it] 52%|█████▏    | 521/1000 [46:14<39:19,  4.93s/it] 52%|█████▏    | 522/1000 [46:16<32:20,  4.06s/it] 52%|█████▏    | 523/1000 [46:22<37:09,  4.67s/it] 52%|█████▏    | 524/1000 [46:26<37:05,  4.68s/it] 52%|█████▎    | 525/1000 [46:31<37:02,  4.68s/it] 53%|█████▎    | 526/1000 [46:36<36:43,  4.65s/it] 53%|█████▎    | 527/1000 [46:41<36:59,  4.69s/it] 53%|█████▎    | 528/1000 [46:45<36:56,  4.70s/it] 53%|█████▎    | 529/1000 [46:50<37:11,  4.74s/it] 53%|█████▎    | 530/1000 [46:55<37:23,  4.77s/it]{'loss': 0.0004, 'grad_norm': 0.02052300050854683, 'learning_rate': 6e-06, 'epoch': 28.89}
Step 530/1000 (53.0%) | Loss: 0.0004                                                   53%|█████▎    | 530/1000 [46:55<37:23,  4.77s/it] 53%|█████▎    | 531/1000 [47:00<37:00,  4.73s/it] 53%|█████▎    | 532/1000 [47:04<37:15,  4.78s/it] 53%|█████▎    | 533/1000 [47:11<40:45,  5.24s/it] 53%|█████▎    | 534/1000 [47:16<39:38,  5.10s/it] 54%|█████▎    | 535/1000 [47:21<39:47,  5.13s/it] 54%|█████▎    | 536/1000 [47:25<38:23,  4.96s/it] 54%|█████▎    | 537/1000 [47:30<37:34,  4.87s/it] 54%|█████▍    | 538/1000 [47:37<41:39,  5.41s/it] 54%|█████▍    | 539/1000 [47:41<39:51,  5.19s/it] 54%|█████▍    | 540/1000 [47:43<32:21,  4.22s/it]{'loss': 0.0004, 'grad_norm': 0.020156798884272575, 'learning_rate': 5.8750000000000005e-06, 'epoch': 29.44}
Step 540/1000 (54.0%) | Loss: 0.0004                                                   54%|█████▍    | 540/1000 [47:43<32:21,  4.22s/it] 54%|█████▍    | 541/1000 [47:49<36:11,  4.73s/it] 54%|█████▍    | 542/1000 [47:54<35:45,  4.68s/it] 54%|█████▍    | 543/1000 [47:59<36:01,  4.73s/it] 54%|█████▍    | 544/1000 [48:03<35:39,  4.69s/it] 55%|█████▍    | 545/1000 [48:08<36:49,  4.86s/it] 55%|█████▍    | 546/1000 [48:13<36:26,  4.82s/it] 55%|█████▍    | 547/1000 [48:18<35:51,  4.75s/it] 55%|█████▍    | 548/1000 [48:23<36:05,  4.79s/it] 55%|█████▍    | 549/1000 [48:28<36:06,  4.80s/it] 55%|█████▌    | 550/1000 [48:32<36:05,  4.81s/it]{'loss': 0.0004, 'grad_norm': 0.0207503791898489, 'learning_rate': 5.75e-06, 'epoch': 30.0}
Step 550/1000 (55.0%) | Loss: 0.0004                                                   55%|█████▌    | 550/1000 [48:32<36:05,  4.81s/it] 55%|█████▌    | 551/1000 [48:37<35:40,  4.77s/it] 55%|█████▌    | 552/1000 [48:42<35:52,  4.80s/it] 55%|█████▌    | 553/1000 [48:47<35:40,  4.79s/it] 55%|█████▌    | 554/1000 [48:51<35:43,  4.81s/it] 56%|█████▌    | 555/1000 [48:56<35:36,  4.80s/it] 56%|█████▌    | 556/1000 [49:01<36:00,  4.87s/it] 56%|█████▌    | 557/1000 [49:06<35:24,  4.80s/it] 56%|█████▌    | 558/1000 [49:08<29:33,  4.01s/it] 56%|█████▌    | 559/1000 [49:14<34:17,  4.67s/it] 56%|█████▌    | 560/1000 [49:19<34:12,  4.67s/it]{'loss': 0.0004, 'grad_norm': 0.01822224073112011, 'learning_rate': 5.625e-06, 'epoch': 30.56}
Step 560/1000 (56.0%) | Loss: 0.0004                                                   56%|█████▌    | 560/1000 [49:19<34:12,  4.67s/it] 56%|█████▌    | 561/1000 [49:24<34:07,  4.66s/it] 56%|█████▌    | 562/1000 [49:30<37:53,  5.19s/it] 56%|█████▋    | 563/1000 [49:35<36:56,  5.07s/it] 56%|█████▋    | 564/1000 [49:40<36:10,  4.98s/it] 56%|█████▋    | 565/1000 [49:44<35:53,  4.95s/it] 57%|█████▋    | 566/1000 [49:49<35:08,  4.86s/it] 57%|█████▋    | 567/1000 [49:54<34:12,  4.74s/it] 57%|█████▋    | 568/1000 [49:58<33:47,  4.69s/it] 57%|█████▋    | 569/1000 [50:03<33:21,  4.64s/it] 57%|█████▋    | 570/1000 [50:07<33:21,  4.65s/it]{'loss': 0.0004, 'grad_norm': 0.015324447304010391, 'learning_rate': 5.500000000000001e-06, 'epoch': 31.11}
Step 570/1000 (57.0%) | Loss: 0.0003                                                   57%|█████▋    | 570/1000 [50:07<33:21,  4.65s/it] 57%|█████▋    | 571/1000 [50:12<33:24,  4.67s/it] 57%|█████▋    | 572/1000 [50:17<34:08,  4.79s/it] 57%|█████▋    | 573/1000 [50:22<33:43,  4.74s/it] 57%|█████▋    | 574/1000 [50:26<33:13,  4.68s/it] 57%|█████▊    | 575/1000 [50:31<33:28,  4.72s/it] 58%|█████▊    | 576/1000 [50:33<27:36,  3.91s/it] 58%|█████▊    | 577/1000 [50:40<32:48,  4.65s/it] 58%|█████▊    | 578/1000 [50:44<32:35,  4.63s/it] 58%|█████▊    | 579/1000 [50:49<32:31,  4.64s/it] 58%|█████▊    | 580/1000 [50:55<34:45,  4.96s/it]{'loss': 0.0003, 'grad_norm': 0.012333295308053493, 'learning_rate': 5.375e-06, 'epoch': 31.67}
Step 580/1000 (58.0%) | Loss: 0.0003                                                   58%|█████▊    | 580/1000 [50:55<34:45,  4.96s/it] 58%|█████▊    | 581/1000 [51:01<37:39,  5.39s/it] 58%|█████▊    | 582/1000 [51:06<36:12,  5.20s/it] 58%|█████▊    | 583/1000 [51:10<34:57,  5.03s/it] 58%|█████▊    | 584/1000 [51:15<33:56,  4.90s/it] 58%|█████▊    | 585/1000 [51:20<33:20,  4.82s/it] 59%|█████▊    | 586/1000 [51:24<33:08,  4.80s/it] 59%|█████▊    | 587/1000 [51:30<34:13,  4.97s/it] 59%|█████▉    | 588/1000 [51:34<33:32,  4.88s/it] 59%|█████▉    | 589/1000 [51:40<34:09,  4.99s/it] 59%|█████▉    | 590/1000 [51:44<32:59,  4.83s/it]{'loss': 0.0003, 'grad_norm': 0.013598631136119366, 'learning_rate': 5.2500000000000006e-06, 'epoch': 32.22}
Step 590/1000 (59.0%) | Loss: 0.0003                                                   59%|█████▉    | 590/1000 [51:44<32:59,  4.83s/it] 59%|█████▉    | 591/1000 [51:49<34:00,  4.99s/it] 59%|█████▉    | 592/1000 [51:54<32:58,  4.85s/it] 59%|█████▉    | 593/1000 [51:58<32:16,  4.76s/it] 59%|█████▉    | 594/1000 [52:01<26:45,  3.96s/it] 60%|█████▉    | 595/1000 [52:06<30:39,  4.54s/it] 60%|█████▉    | 596/1000 [52:12<32:42,  4.86s/it] 60%|█████▉    | 597/1000 [52:18<34:19,  5.11s/it] 60%|█████▉    | 598/1000 [52:22<33:07,  4.95s/it] 60%|█████▉    | 599/1000 [52:28<33:35,  5.03s/it] 60%|██████    | 600/1000 [52:32<32:42,  4.91s/it]{'loss': 0.0003, 'grad_norm': 0.015047899447381496, 'learning_rate': 5.125e-06, 'epoch': 32.78}
Step 600/1000 (60.0%) | Loss: 0.0003                                                   60%|██████    | 600/1000 [52:32<32:42,  4.91s/it] 60%|██████    | 601/1000 [52:37<32:53,  4.95s/it] 60%|██████    | 602/1000 [52:43<33:42,  5.08s/it] 60%|██████    | 603/1000 [52:47<33:10,  5.01s/it] 60%|██████    | 604/1000 [52:52<32:28,  4.92s/it] 60%|██████    | 605/1000 [52:57<31:36,  4.80s/it] 61%|██████    | 606/1000 [53:02<33:29,  5.10s/it] 61%|██████    | 607/1000 [53:07<32:50,  5.01s/it] 61%|██████    | 608/1000 [53:13<35:08,  5.38s/it] 61%|██████    | 609/1000 [53:20<37:17,  5.72s/it] 61%|██████    | 610/1000 [53:25<35:01,  5.39s/it]{'loss': 0.0003, 'grad_norm': 0.014563167467713356, 'learning_rate': 5e-06, 'epoch': 33.33}
Step 610/1000 (61.0%) | Loss: 0.0003                                                   61%|██████    | 610/1000 [53:25<35:01,  5.39s/it] 61%|██████    | 611/1000 [53:29<33:27,  5.16s/it] 61%|██████    | 612/1000 [53:31<27:14,  4.21s/it] 61%|██████▏   | 613/1000 [53:38<32:35,  5.05s/it] 61%|██████▏   | 614/1000 [53:43<31:49,  4.95s/it] 62%|██████▏   | 615/1000 [53:48<31:16,  4.87s/it] 62%|██████▏   | 616/1000 [53:53<32:55,  5.14s/it] 62%|██████▏   | 617/1000 [53:58<31:46,  4.98s/it] 62%|██████▏   | 618/1000 [54:03<32:18,  5.07s/it] 62%|██████▏   | 619/1000 [54:08<31:09,  4.91s/it] 62%|██████▏   | 620/1000 [54:13<30:46,  4.86s/it]{'loss': 0.0003, 'grad_norm': 0.011596573516726494, 'learning_rate': 4.875e-06, 'epoch': 33.89}
Step 620/1000 (62.0%) | Loss: 0.0003                                                   62%|██████▏   | 620/1000 [54:13<30:46,  4.86s/it] 62%|██████▏   | 621/1000 [54:19<34:04,  5.39s/it] 62%|██████▏   | 622/1000 [54:24<32:36,  5.18s/it] 62%|██████▏   | 623/1000 [54:29<31:45,  5.05s/it] 62%|██████▏   | 624/1000 [54:34<31:37,  5.05s/it] 62%|██████▎   | 625/1000 [54:38<30:40,  4.91s/it] 63%|██████▎   | 626/1000 [54:43<30:10,  4.84s/it] 63%|██████▎   | 627/1000 [54:47<29:28,  4.74s/it] 63%|██████▎   | 628/1000 [54:53<30:01,  4.84s/it] 63%|██████▎   | 629/1000 [54:59<32:20,  5.23s/it] 63%|██████▎   | 630/1000 [55:01<26:17,  4.26s/it]{'loss': 0.0003, 'grad_norm': 0.014798562973737717, 'learning_rate': 4.75e-06, 'epoch': 34.44}
Step 630/1000 (63.0%) | Loss: 0.0003                                                   63%|██████▎   | 630/1000 [55:01<26:17,  4.26s/it] 63%|██████▎   | 631/1000 [55:07<30:30,  4.96s/it] 63%|██████▎   | 632/1000 [55:12<30:42,  5.01s/it] 63%|██████▎   | 633/1000 [55:17<30:40,  5.01s/it] 63%|██████▎   | 634/1000 [55:22<29:54,  4.90s/it] 64%|██████▎   | 635/1000 [55:28<31:49,  5.23s/it] 64%|██████▎   | 636/1000 [55:34<33:12,  5.47s/it] 64%|██████▎   | 637/1000 [55:40<33:42,  5.57s/it] 64%|██████▍   | 638/1000 [55:46<33:52,  5.61s/it] 64%|██████▍   | 639/1000 [55:50<32:08,  5.34s/it] 64%|██████▍   | 640/1000 [55:56<32:32,  5.42s/it]{'loss': 0.0003, 'grad_norm': 0.012920894660055637, 'learning_rate': 4.625000000000001e-06, 'epoch': 35.0}
Step 640/1000 (64.0%) | Loss: 0.0003                                                   64%|██████▍   | 640/1000 [55:56<32:32,  5.42s/it] 64%|██████▍   | 641/1000 [56:02<33:57,  5.68s/it] 64%|██████▍   | 642/1000 [56:07<31:48,  5.33s/it] 64%|██████▍   | 643/1000 [56:12<32:26,  5.45s/it] 64%|██████▍   | 644/1000 [56:18<31:59,  5.39s/it] 64%|██████▍   | 645/1000 [56:22<30:28,  5.15s/it] 65%|██████▍   | 646/1000 [56:27<29:21,  4.98s/it] 65%|██████▍   | 647/1000 [56:32<28:41,  4.88s/it] 65%|██████▍   | 648/1000 [56:34<23:33,  4.02s/it] 65%|██████▍   | 649/1000 [56:40<27:08,  4.64s/it] 65%|██████▌   | 650/1000 [56:44<27:03,  4.64s/it]{'loss': 0.0003, 'grad_norm': 0.01349043007940054, 'learning_rate': 4.5e-06, 'epoch': 35.56}
Step 650/1000 (65.0%) | Loss: 0.0003                                                   65%|██████▌   | 650/1000 [56:44<27:03,  4.64s/it] 65%|██████▌   | 651/1000 [56:50<28:22,  4.88s/it] 65%|██████▌   | 652/1000 [56:57<31:44,  5.47s/it] 65%|██████▌   | 653/1000 [57:01<30:21,  5.25s/it] 65%|██████▌   | 654/1000 [57:06<29:09,  5.06s/it] 66%|██████▌   | 655/1000 [57:11<29:30,  5.13s/it] 66%|██████▌   | 656/1000 [57:16<28:48,  5.02s/it] 66%|██████▌   | 657/1000 [57:21<28:22,  4.96s/it] 66%|██████▌   | 658/1000 [57:25<27:27,  4.82s/it] 66%|██████▌   | 659/1000 [57:30<27:00,  4.75s/it] 66%|██████▌   | 660/1000 [57:35<26:58,  4.76s/it]{'loss': 0.0003, 'grad_norm': 0.010312353260815144, 'learning_rate': 4.3750000000000005e-06, 'epoch': 36.11}
Step 660/1000 (66.0%) | Loss: 0.0003                                                   66%|██████▌   | 660/1000 [57:35<26:58,  4.76s/it] 66%|██████▌   | 661/1000 [57:39<26:43,  4.73s/it] 66%|██████▌   | 662/1000 [57:44<27:09,  4.82s/it] 66%|██████▋   | 663/1000 [57:51<29:54,  5.33s/it] 66%|██████▋   | 664/1000 [57:55<28:38,  5.11s/it] 66%|██████▋   | 665/1000 [58:00<27:51,  4.99s/it] 67%|██████▋   | 666/1000 [58:02<22:50,  4.10s/it] 67%|██████▋   | 667/1000 [58:08<26:01,  4.69s/it] 67%|██████▋   | 668/1000 [58:14<27:40,  5.00s/it] 67%|██████▋   | 669/1000 [58:19<26:49,  4.86s/it] 67%|██████▋   | 670/1000 [58:23<26:18,  4.78s/it]{'loss': 0.0003, 'grad_norm': 0.010596226900815964, 'learning_rate': 4.25e-06, 'epoch': 36.67}
Step 670/1000 (67.0%) | Loss: 0.0003                                                   67%|██████▋   | 670/1000 [58:23<26:18,  4.78s/it] 67%|██████▋   | 671/1000 [58:28<26:51,  4.90s/it] 67%|██████▋   | 672/1000 [58:33<26:45,  4.90s/it] 67%|██████▋   | 673/1000 [58:38<26:33,  4.87s/it] 67%|██████▋   | 674/1000 [58:43<26:02,  4.79s/it] 68%|██████▊   | 675/1000 [58:47<25:39,  4.74s/it] 68%|██████▊   | 676/1000 [58:52<25:38,  4.75s/it] 68%|██████▊   | 677/1000 [58:57<25:17,  4.70s/it] 68%|██████▊   | 678/1000 [59:01<25:03,  4.67s/it] 68%|██████▊   | 679/1000 [59:06<24:56,  4.66s/it] 68%|██████▊   | 680/1000 [59:13<28:05,  5.27s/it]{'loss': 0.0003, 'grad_norm': 0.011483702808618546, 'learning_rate': 4.125e-06, 'epoch': 37.22}
Step 680/1000 (68.0%) | Loss: 0.0003                                                   68%|██████▊   | 680/1000 [59:13<28:05,  5.27s/it] 68%|██████▊   | 681/1000 [59:17<27:29,  5.17s/it] 68%|██████▊   | 682/1000 [59:23<27:15,  5.14s/it] 68%|██████▊   | 683/1000 [59:27<26:37,  5.04s/it] 68%|██████▊   | 684/1000 [59:30<22:19,  4.24s/it] 68%|██████▊   | 685/1000 [59:37<27:48,  5.30s/it] 69%|██████▊   | 686/1000 [59:43<27:40,  5.29s/it] 69%|██████▊   | 687/1000 [59:49<29:11,  5.59s/it] 69%|██████▉   | 688/1000 [59:55<29:19,  5.64s/it] 69%|██████▉   | 689/1000 [59:59<27:31,  5.31s/it] 69%|██████▉   | 690/1000 [1:00:04<26:13,  5.08s/it]{'loss': 0.0003, 'grad_norm': 0.009802115149796009, 'learning_rate': 4.000000000000001e-06, 'epoch': 37.78}
Step 690/1000 (69.0%) | Loss: 0.0003                                                     69%|██████▉   | 690/1000 [1:00:04<26:13,  5.08s/it] 69%|██████▉   | 691/1000 [1:00:09<25:30,  4.95s/it] 69%|██████▉   | 692/1000 [1:00:14<25:58,  5.06s/it] 69%|██████▉   | 693/1000 [1:00:19<26:37,  5.20s/it] 69%|██████▉   | 694/1000 [1:00:24<25:43,  5.04s/it] 70%|██████▉   | 695/1000 [1:00:29<24:55,  4.90s/it] 70%|██████▉   | 696/1000 [1:00:33<24:44,  4.88s/it] 70%|██████▉   | 697/1000 [1:00:39<25:16,  5.00s/it] 70%|██████▉   | 698/1000 [1:00:43<24:44,  4.91s/it] 70%|██████▉   | 699/1000 [1:00:48<24:19,  4.85s/it] 70%|███████   | 700/1000 [1:00:53<24:29,  4.90s/it]{'loss': 0.0003, 'grad_norm': 0.009840549901127815, 'learning_rate': 3.875e-06, 'epoch': 38.33}
Step 700/1000 (70.0%) | Loss: 0.0003                                                     70%|███████   | 700/1000 [1:00:53<24:29,  4.90s/it] 70%|███████   | 701/1000 [1:00:58<24:18,  4.88s/it] 70%|███████   | 702/1000 [1:01:00<20:08,  4.05s/it] 70%|███████   | 703/1000 [1:01:07<24:11,  4.89s/it] 70%|███████   | 704/1000 [1:01:12<23:42,  4.81s/it] 70%|███████   | 705/1000 [1:01:16<23:28,  4.78s/it] 71%|███████   | 706/1000 [1:01:21<23:40,  4.83s/it] 71%|███████   | 707/1000 [1:01:26<23:31,  4.82s/it] 71%|███████   | 708/1000 [1:01:31<23:11,  4.77s/it] 71%|███████   | 709/1000 [1:01:35<22:54,  4.72s/it] 71%|███████   | 710/1000 [1:01:41<24:39,  5.10s/it]{'loss': 0.0003, 'grad_norm': 0.01003280933946371, 'learning_rate': 3.7500000000000005e-06, 'epoch': 38.89}
Step 710/1000 (71.0%) | Loss: 0.0002                                                     71%|███████   | 710/1000 [1:01:41<24:39,  5.10s/it] 71%|███████   | 711/1000 [1:01:46<23:51,  4.95s/it] 71%|███████   | 712/1000 [1:01:51<23:37,  4.92s/it] 71%|███████▏  | 713/1000 [1:01:57<24:48,  5.19s/it] 71%|███████▏  | 714/1000 [1:02:02<24:58,  5.24s/it] 72%|███████▏  | 715/1000 [1:02:06<23:55,  5.04s/it] 72%|███████▏  | 716/1000 [1:02:12<24:17,  5.13s/it] 72%|███████▏  | 717/1000 [1:02:16<23:26,  4.97s/it] 72%|███████▏  | 718/1000 [1:02:21<23:08,  4.92s/it] 72%|███████▏  | 719/1000 [1:02:26<22:41,  4.85s/it] 72%|███████▏  | 720/1000 [1:02:28<18:39,  4.00s/it]{'loss': 0.0002, 'grad_norm': 0.010089215822517872, 'learning_rate': 3.625e-06, 'epoch': 39.44}
Step 720/1000 (72.0%) | Loss: 0.0002                                                     72%|███████▏  | 720/1000 [1:02:28<18:39,  4.00s/it] 72%|███████▏  | 721/1000 [1:02:34<21:33,  4.63s/it] 72%|███████▏  | 722/1000 [1:02:40<23:05,  4.98s/it] 72%|███████▏  | 723/1000 [1:02:44<22:23,  4.85s/it] 72%|███████▏  | 724/1000 [1:02:50<22:56,  4.99s/it] 72%|███████▎  | 725/1000 [1:02:54<22:12,  4.85s/it] 73%|███████▎  | 726/1000 [1:02:59<21:40,  4.75s/it] 73%|███████▎  | 727/1000 [1:03:03<21:35,  4.75s/it] 73%|███████▎  | 728/1000 [1:03:08<21:23,  4.72s/it] 73%|███████▎  | 729/1000 [1:03:13<21:29,  4.76s/it] 73%|███████▎  | 730/1000 [1:03:18<21:10,  4.71s/it]{'loss': 0.0002, 'grad_norm': 0.012295085936784744, 'learning_rate': 3.5e-06, 'epoch': 40.0}
Step 730/1000 (73.0%) | Loss: 0.0002                                                     73%|███████▎  | 730/1000 [1:03:18<21:10,  4.71s/it] 73%|███████▎  | 731/1000 [1:03:22<20:50,  4.65s/it] 73%|███████▎  | 732/1000 [1:03:28<21:52,  4.90s/it] 73%|███████▎  | 733/1000 [1:03:32<21:21,  4.80s/it] 73%|███████▎  | 734/1000 [1:03:37<21:18,  4.81s/it] 74%|███████▎  | 735/1000 [1:03:42<22:01,  4.99s/it] 74%|███████▎  | 736/1000 [1:03:47<21:29,  4.89s/it] 74%|███████▎  | 737/1000 [1:03:52<21:53,  5.00s/it] 74%|███████▍  | 738/1000 [1:03:54<18:03,  4.13s/it] 74%|███████▍  | 739/1000 [1:04:00<20:07,  4.63s/it] 74%|███████▍  | 740/1000 [1:04:05<20:26,  4.72s/it]{'loss': 0.0002, 'grad_norm': 0.00941027607768774, 'learning_rate': 3.3750000000000003e-06, 'epoch': 40.56}
Step 740/1000 (74.0%) | Loss: 0.0002                                                     74%|███████▍  | 740/1000 [1:04:05<20:26,  4.72s/it] 74%|███████▍  | 741/1000 [1:04:10<20:18,  4.70s/it] 74%|███████▍  | 742/1000 [1:04:15<20:31,  4.77s/it] 74%|███████▍  | 743/1000 [1:04:19<20:04,  4.69s/it] 74%|███████▍  | 744/1000 [1:04:25<21:20,  5.00s/it] 74%|███████▍  | 745/1000 [1:04:30<21:59,  5.17s/it] 75%|███████▍  | 746/1000 [1:04:35<21:18,  5.03s/it] 75%|███████▍  | 747/1000 [1:04:40<21:12,  5.03s/it] 75%|███████▍  | 748/1000 [1:04:45<20:38,  4.91s/it] 75%|███████▍  | 749/1000 [1:04:50<20:28,  4.89s/it] 75%|███████▌  | 750/1000 [1:04:55<20:20,  4.88s/it]{'loss': 0.0002, 'grad_norm': 0.009826275520026684, 'learning_rate': 3.2500000000000002e-06, 'epoch': 41.11}
Step 750/1000 (75.0%) | Loss: 0.0002                                                     75%|███████▌  | 750/1000 [1:04:55<20:20,  4.88s/it]{'loss': 0.0002, 'grad_norm': 0.008644592016935349, 'learning_rate': 3.125e-06, 'epoch': 41.67}

  0%|          | 0/49 [00:00<?, ?it/s][A
  4%|▍         | 2/49 [00:01<00:42,  1.11it/s][A
  6%|▌         | 3/49 [00:03<00:52,  1.13s/it][A
  8%|▊         | 4/49 [00:04<00:55,  1.24s/it][A
 10%|█         | 5/49 [00:05<00:55,  1.26s/it][A
 12%|█▏        | 6/49 [00:07<01:04,  1.50s/it][A
 14%|█▍        | 7/49 [00:10<01:10,  1.68s/it][A
 16%|█▋        | 8/49 [00:11<01:06,  1.62s/it][A
 18%|█▊        | 9/49 [00:13<01:07,  1.69s/it][A
 20%|██        | 10/49 [00:15<01:07,  1.72s/it][A
 22%|██▏       | 11/49 [00:16<01:04,  1.70s/it][A
 24%|██▍       | 12/49 [00:18<00:59,  1.61s/it][A
 27%|██▋       | 13/49 [00:20<01:00,  1.68s/it][A
 29%|██▊       | 14/49 [00:21<00:56,  1.63s/it][A
 31%|███       | 15/49 [00:23<00:54,  1.62s/it][A
 33%|███▎      | 16/49 [00:24<00:51,  1.57s/it][A
 35%|███▍      | 17/49 [00:26<00:50,  1.57s/it][A
 37%|███▋      | 18/49 [00:27<00:49,  1.60s/it][A
 39%|███▉      | 19/49 [00:29<00:46,  1.55s/it][A
 41%|████      | 20/49 [00:31<00:49,  1.70s/it][A
 43%|████▎     | 21/49 [00:33<00:53,  1.92s/it][A
 45%|████▍     | 22/49 [00:35<00:54,  2.01s/it][A
 47%|████▋     | 23/49 [00:37<00:48,  1.87s/it][A
 49%|████▉     | 24/49 [00:39<00:45,  1.82s/it][A
 51%|█████     | 25/49 [00:41<00:47,  1.97s/it][A
 53%|█████▎    | 26/49 [00:42<00:41,  1.79s/it][A
 55%|█████▌    | 27/49 [00:44<00:37,  1.70s/it][A
 57%|█████▋    | 28/49 [00:45<00:34,  1.63s/it][A
 59%|█████▉    | 29/49 [00:47<00:29,  1.50s/it][A
 61%|██████    | 30/49 [00:48<00:29,  1.57s/it][A
 63%|██████▎   | 31/49 [00:50<00:30,  1.71s/it][A
 65%|██████▌   | 32/49 [00:52<00:27,  1.63s/it][A
 67%|██████▋   | 33/49 [00:53<00:25,  1.60s/it][A
 69%|██████▉   | 34/49 [00:56<00:27,  1.82s/it][A
 71%|███████▏  | 35/49 [00:58<00:25,  1.84s/it][A
 73%|███████▎  | 36/49 [00:59<00:22,  1.75s/it][A
 76%|███████▌  | 37/49 [01:02<00:23,  1.98s/it][A
 78%|███████▊  | 38/49 [01:04<00:24,  2.20s/it][A
 80%|███████▉  | 39/49 [01:08<00:26,  2.61s/it][A
 82%|████████▏ | 40/49 [01:11<00:24,  2.76s/it][A
 84%|████████▎ | 41/49 [01:14<00:22,  2.79s/it][A
 86%|████████▌ | 42/49 [01:16<00:18,  2.63s/it][A
 88%|████████▊ | 43/49 [01:18<00:14,  2.37s/it][A
 90%|████████▉ | 44/49 [01:20<00:11,  2.27s/it][A
 92%|█████████▏| 45/49 [01:21<00:08,  2.02s/it][A
 94%|█████████▍| 46/49 [01:23<00:05,  1.84s/it][A
 96%|█████████▌| 47/49 [01:24<00:03,  1.74s/it][A
 98%|█████████▊| 48/49 [01:25<00:01,  1.52s/it][A
100%|██████████| 49/49 [01:26<00:00,  1.18s/it][A                                                    
                                               [A 75%|███████▌  | 750/1000 [1:06:41<20:20,  4.88s/it]
100%|██████████| 49/49 [01:44<00:00,  1.18s/it][A
                                               [ASome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}
/home/ashish/common_voice_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 75%|███████▌  | 751/1000 [1:06:54<2:42:40, 39.20s/it] 75%|███████▌  | 752/1000 [1:07:00<2:00:41, 29.20s/it] 75%|███████▌  | 753/1000 [1:07:05<1:30:08, 21.90s/it] 75%|███████▌  | 754/1000 [1:07:09<1:08:48, 16.78s/it] 76%|███████▌  | 755/1000 [1:07:14<53:55, 13.21s/it]   76%|███████▌  | 756/1000 [1:07:16<40:00,  9.84s/it] 76%|███████▌  | 757/1000 [1:07:22<34:57,  8.63s/it] 76%|███████▌  | 758/1000 [1:07:27<30:50,  7.65s/it] 76%|███████▌  | 759/1000 [1:07:32<27:14,  6.78s/it] 76%|███████▌  | 760/1000 [1:07:37<24:37,  6.15s/it]{'eval_loss': 0.8896823525428772, 'eval_wer': 68.78980891719745, 'eval_runtime': 106.9051, 'eval_samples_per_second': 1.805, 'eval_steps_per_second': 0.458, 'epoch': 41.67}
Step 760/1000 (76.0%) | Loss: 0.0002                                                     76%|███████▌  | 760/1000 [1:07:37<24:37,  6.15s/it] 76%|███████▌  | 761/1000 [1:07:41<22:43,  5.70s/it] 76%|███████▌  | 762/1000 [1:07:46<21:17,  5.37s/it] 76%|███████▋  | 763/1000 [1:07:51<20:37,  5.22s/it] 76%|███████▋  | 764/1000 [1:07:56<19:53,  5.06s/it] 76%|███████▋  | 765/1000 [1:08:00<19:28,  4.97s/it] 77%|███████▋  | 766/1000 [1:08:05<18:49,  4.83s/it] 77%|███████▋  | 767/1000 [1:08:09<18:27,  4.75s/it] 77%|███████▋  | 768/1000 [1:08:14<18:12,  4.71s/it] 77%|███████▋  | 769/1000 [1:08:20<19:21,  5.03s/it] 77%|███████▋  | 770/1000 [1:08:25<19:09,  5.00s/it]{'loss': 0.0002, 'grad_norm': 0.010266786441206932, 'learning_rate': 3e-06, 'epoch': 42.22}
Step 770/1000 (77.0%) | Loss: 0.0002                                                     77%|███████▋  | 770/1000 [1:08:25<19:09,  5.00s/it] 77%|███████▋  | 771/1000 [1:08:29<18:40,  4.89s/it] 77%|███████▋  | 772/1000 [1:08:34<18:29,  4.87s/it] 77%|███████▋  | 773/1000 [1:08:41<20:06,  5.31s/it] 77%|███████▋  | 774/1000 [1:08:43<16:12,  4.30s/it] 78%|███████▊  | 775/1000 [1:08:48<17:53,  4.77s/it] 78%|███████▊  | 776/1000 [1:08:53<17:48,  4.77s/it] 78%|███████▊  | 777/1000 [1:08:58<17:39,  4.75s/it] 78%|███████▊  | 778/1000 [1:09:03<17:40,  4.77s/it] 78%|███████▊  | 779/1000 [1:09:07<17:31,  4.76s/it] 78%|███████▊  | 780/1000 [1:09:12<17:25,  4.75s/it]{'loss': 0.0002, 'grad_norm': 0.008752172812819481, 'learning_rate': 2.875e-06, 'epoch': 42.78}
Step 780/1000 (78.0%) | Loss: 0.0002                                                     78%|███████▊  | 780/1000 [1:09:12<17:25,  4.75s/it] 78%|███████▊  | 781/1000 [1:09:17<17:14,  4.72s/it] 78%|███████▊  | 782/1000 [1:09:21<17:00,  4.68s/it] 78%|███████▊  | 783/1000 [1:09:26<16:52,  4.67s/it] 78%|███████▊  | 784/1000 [1:09:31<16:52,  4.69s/it] 78%|███████▊  | 785/1000 [1:09:36<16:58,  4.74s/it] 79%|███████▊  | 786/1000 [1:09:40<16:59,  4.76s/it] 79%|███████▊  | 787/1000 [1:09:45<16:40,  4.70s/it] 79%|███████▉  | 788/1000 [1:09:50<16:31,  4.68s/it] 79%|███████▉  | 789/1000 [1:09:54<16:28,  4.68s/it] 79%|███████▉  | 790/1000 [1:10:00<17:23,  4.97s/it]{'loss': 0.0002, 'grad_norm': 0.007208542432636023, 'learning_rate': 2.7500000000000004e-06, 'epoch': 43.33}
Step 790/1000 (79.0%) | Loss: 0.0002                                                     79%|███████▉  | 790/1000 [1:10:00<17:23,  4.97s/it] 79%|███████▉  | 791/1000 [1:10:05<17:04,  4.90s/it] 79%|███████▉  | 792/1000 [1:10:07<13:55,  4.02s/it] 79%|███████▉  | 793/1000 [1:10:13<16:11,  4.69s/it] 79%|███████▉  | 794/1000 [1:10:18<16:07,  4.70s/it] 80%|███████▉  | 795/1000 [1:10:22<16:03,  4.70s/it] 80%|███████▉  | 796/1000 [1:10:28<17:22,  5.11s/it] 80%|███████▉  | 797/1000 [1:10:33<16:57,  5.01s/it] 80%|███████▉  | 798/1000 [1:10:38<16:43,  4.97s/it] 80%|███████▉  | 799/1000 [1:10:43<16:13,  4.84s/it] 80%|████████  | 800/1000 [1:10:47<16:06,  4.83s/it]{'loss': 0.0002, 'grad_norm': 0.00830072071403265, 'learning_rate': 2.6250000000000003e-06, 'epoch': 43.89}
Step 800/1000 (80.0%) | Loss: 0.0002                                                     80%|████████  | 800/1000 [1:10:47<16:06,  4.83s/it] 80%|████████  | 801/1000 [1:10:52<16:06,  4.86s/it] 80%|████████  | 802/1000 [1:10:57<15:48,  4.79s/it] 80%|████████  | 803/1000 [1:11:02<15:45,  4.80s/it] 80%|████████  | 804/1000 [1:11:07<15:46,  4.83s/it] 80%|████████  | 805/1000 [1:11:13<16:55,  5.21s/it] 81%|████████  | 806/1000 [1:11:17<16:17,  5.04s/it] 81%|████████  | 807/1000 [1:11:22<15:58,  4.97s/it] 81%|████████  | 808/1000 [1:11:27<15:40,  4.90s/it] 81%|████████  | 809/1000 [1:11:32<15:37,  4.91s/it] 81%|████████  | 810/1000 [1:11:34<12:48,  4.04s/it]{'loss': 0.0002, 'grad_norm': 0.011913319118320942, 'learning_rate': 2.5e-06, 'epoch': 44.44}
Step 810/1000 (81.0%) | Loss: 0.0002                                                     81%|████████  | 810/1000 [1:11:34<12:48,  4.04s/it] 81%|████████  | 811/1000 [1:11:40<14:22,  4.56s/it] 81%|████████  | 812/1000 [1:11:44<14:18,  4.56s/it] 81%|████████▏ | 813/1000 [1:11:49<14:13,  4.56s/it] 81%|████████▏ | 814/1000 [1:11:54<14:23,  4.64s/it] 82%|████████▏ | 815/1000 [1:12:00<15:31,  5.03s/it] 82%|████████▏ | 816/1000 [1:12:04<15:12,  4.96s/it] 82%|████████▏ | 817/1000 [1:12:09<14:52,  4.88s/it] 82%|████████▏ | 818/1000 [1:12:15<15:23,  5.07s/it] 82%|████████▏ | 819/1000 [1:12:21<16:13,  5.38s/it] 82%|████████▏ | 820/1000 [1:12:25<15:22,  5.12s/it]{'loss': 0.0002, 'grad_norm': 0.010893476195633411, 'learning_rate': 2.375e-06, 'epoch': 45.0}
Step 820/1000 (82.0%) | Loss: 0.0002                                                     82%|████████▏ | 820/1000 [1:12:25<15:22,  5.12s/it] 82%|████████▏ | 821/1000 [1:12:31<16:14,  5.45s/it] 82%|████████▏ | 822/1000 [1:12:37<15:58,  5.38s/it] 82%|████████▏ | 823/1000 [1:12:41<15:18,  5.19s/it] 82%|████████▏ | 824/1000 [1:12:46<14:39,  5.00s/it] 82%|████████▎ | 825/1000 [1:12:51<14:18,  4.91s/it] 83%|████████▎ | 826/1000 [1:12:56<14:54,  5.14s/it] 83%|████████▎ | 827/1000 [1:13:01<14:22,  4.99s/it] 83%|████████▎ | 828/1000 [1:13:03<11:42,  4.09s/it] 83%|████████▎ | 829/1000 [1:13:09<13:10,  4.62s/it] 83%|████████▎ | 830/1000 [1:13:14<13:18,  4.70s/it]{'loss': 0.0002, 'grad_norm': 0.008702067658305168, 'learning_rate': 2.25e-06, 'epoch': 45.56}
Step 830/1000 (83.0%) | Loss: 0.0002                                                     83%|████████▎ | 830/1000 [1:13:14<13:18,  4.70s/it] 83%|████████▎ | 831/1000 [1:13:18<13:14,  4.70s/it] 83%|████████▎ | 832/1000 [1:13:23<13:17,  4.75s/it] 83%|████████▎ | 833/1000 [1:13:28<13:38,  4.90s/it] 83%|████████▎ | 834/1000 [1:13:33<13:16,  4.80s/it] 84%|████████▎ | 835/1000 [1:13:38<13:03,  4.75s/it] 84%|████████▎ | 836/1000 [1:13:43<13:38,  4.99s/it] 84%|████████▎ | 837/1000 [1:13:48<13:18,  4.90s/it] 84%|████████▍ | 838/1000 [1:13:53<13:00,  4.82s/it] 84%|████████▍ | 839/1000 [1:13:57<12:41,  4.73s/it] 84%|████████▍ | 840/1000 [1:14:02<12:28,  4.68s/it]{'loss': 0.0002, 'grad_norm': 0.008057537488639355, 'learning_rate': 2.125e-06, 'epoch': 46.11}
Step 840/1000 (84.0%) | Loss: 0.0002                                                     84%|████████▍ | 840/1000 [1:14:02<12:28,  4.68s/it] 84%|████████▍ | 841/1000 [1:14:07<12:45,  4.81s/it] 84%|████████▍ | 842/1000 [1:14:11<12:36,  4.79s/it] 84%|████████▍ | 843/1000 [1:14:16<12:28,  4.77s/it] 84%|████████▍ | 844/1000 [1:14:21<12:43,  4.90s/it] 84%|████████▍ | 845/1000 [1:14:26<12:29,  4.83s/it] 85%|████████▍ | 846/1000 [1:14:28<10:13,  3.99s/it] 85%|████████▍ | 847/1000 [1:14:34<11:41,  4.58s/it] 85%|████████▍ | 848/1000 [1:14:39<11:54,  4.70s/it] 85%|████████▍ | 849/1000 [1:14:47<14:36,  5.80s/it] 85%|████████▌ | 850/1000 [1:14:52<13:45,  5.51s/it]{'loss': 0.0002, 'grad_norm': 0.007513247895985842, 'learning_rate': 2.0000000000000003e-06, 'epoch': 46.67}
Step 850/1000 (85.0%) | Loss: 0.0002                                                     85%|████████▌ | 850/1000 [1:14:52<13:45,  5.51s/it] 85%|████████▌ | 851/1000 [1:14:58<13:43,  5.52s/it] 85%|████████▌ | 852/1000 [1:15:03<13:21,  5.41s/it] 85%|████████▌ | 853/1000 [1:15:07<12:36,  5.15s/it] 85%|████████▌ | 854/1000 [1:15:12<12:05,  4.97s/it] 86%|████████▌ | 855/1000 [1:15:17<12:00,  4.97s/it] 86%|████████▌ | 856/1000 [1:15:22<11:42,  4.88s/it] 86%|████████▌ | 857/1000 [1:15:26<11:27,  4.81s/it] 86%|████████▌ | 858/1000 [1:15:31<11:25,  4.83s/it] 86%|████████▌ | 859/1000 [1:15:36<11:12,  4.77s/it] 86%|████████▌ | 860/1000 [1:15:42<12:13,  5.24s/it]{'loss': 0.0002, 'grad_norm': 0.007983350194990635, 'learning_rate': 1.8750000000000003e-06, 'epoch': 47.22}
Step 860/1000 (86.0%) | Loss: 0.0002                                                     86%|████████▌ | 860/1000 [1:15:42<12:13,  5.24s/it] 86%|████████▌ | 861/1000 [1:15:47<11:47,  5.09s/it] 86%|████████▌ | 862/1000 [1:15:53<12:28,  5.42s/it] 86%|████████▋ | 863/1000 [1:15:58<12:07,  5.31s/it] 86%|████████▋ | 864/1000 [1:16:00<09:49,  4.34s/it] 86%|████████▋ | 865/1000 [1:16:08<11:58,  5.32s/it] 87%|████████▋ | 866/1000 [1:16:15<12:55,  5.78s/it] 87%|████████▋ | 867/1000 [1:16:20<12:11,  5.50s/it] 87%|████████▋ | 868/1000 [1:16:24<11:32,  5.25s/it] 87%|████████▋ | 869/1000 [1:16:29<11:05,  5.08s/it] 87%|████████▋ | 870/1000 [1:16:34<10:50,  5.00s/it]{'loss': 0.0002, 'grad_norm': 0.008686770685017109, 'learning_rate': 1.75e-06, 'epoch': 47.78}
Step 870/1000 (87.0%) | Loss: 0.0002                                                     87%|████████▋ | 870/1000 [1:16:34<10:50,  5.00s/it] 87%|████████▋ | 871/1000 [1:16:38<10:37,  4.94s/it] 87%|████████▋ | 872/1000 [1:16:43<10:23,  4.87s/it] 87%|████████▋ | 873/1000 [1:16:49<10:41,  5.05s/it] 87%|████████▋ | 874/1000 [1:16:55<11:16,  5.37s/it] 88%|████████▊ | 875/1000 [1:17:00<10:51,  5.21s/it] 88%|████████▊ | 876/1000 [1:17:04<10:28,  5.07s/it] 88%|████████▊ | 877/1000 [1:17:09<10:06,  4.93s/it] 88%|████████▊ | 878/1000 [1:17:15<10:24,  5.12s/it] 88%|████████▊ | 879/1000 [1:17:21<11:08,  5.53s/it] 88%|████████▊ | 880/1000 [1:17:26<10:31,  5.26s/it]{'loss': 0.0002, 'grad_norm': 0.009748750366270542, 'learning_rate': 1.6250000000000001e-06, 'epoch': 48.33}
Step 880/1000 (88.0%) | Loss: 0.0002                                                     88%|████████▊ | 880/1000 [1:17:26<10:31,  5.26s/it] 88%|████████▊ | 881/1000 [1:17:30<10:00,  5.05s/it] 88%|████████▊ | 882/1000 [1:17:32<08:08,  4.14s/it] 88%|████████▊ | 883/1000 [1:17:38<09:01,  4.63s/it] 88%|████████▊ | 884/1000 [1:17:43<08:57,  4.63s/it] 88%|████████▊ | 885/1000 [1:17:48<09:19,  4.87s/it] 89%|████████▊ | 886/1000 [1:17:54<09:48,  5.17s/it] 89%|████████▊ | 887/1000 [1:17:59<09:30,  5.05s/it] 89%|████████▉ | 888/1000 [1:18:03<09:16,  4.97s/it] 89%|████████▉ | 889/1000 [1:18:08<08:58,  4.86s/it] 89%|████████▉ | 890/1000 [1:18:13<09:00,  4.92s/it]{'loss': 0.0002, 'grad_norm': 0.007193780038505793, 'learning_rate': 1.5e-06, 'epoch': 48.89}
Step 890/1000 (89.0%) | Loss: 0.0002                                                     89%|████████▉ | 890/1000 [1:18:13<09:00,  4.92s/it] 89%|████████▉ | 891/1000 [1:18:18<08:52,  4.89s/it] 89%|████████▉ | 892/1000 [1:18:23<08:44,  4.86s/it] 89%|████████▉ | 893/1000 [1:18:27<08:33,  4.80s/it] 89%|████████▉ | 894/1000 [1:18:32<08:23,  4.75s/it] 90%|████████▉ | 895/1000 [1:18:37<08:22,  4.79s/it] 90%|████████▉ | 896/1000 [1:18:42<08:16,  4.78s/it] 90%|████████▉ | 897/1000 [1:18:46<08:05,  4.72s/it] 90%|████████▉ | 898/1000 [1:18:51<08:11,  4.82s/it] 90%|████████▉ | 899/1000 [1:18:56<08:16,  4.92s/it] 90%|█████████ | 900/1000 [1:18:59<06:46,  4.06s/it]{'loss': 0.0002, 'grad_norm': 0.007278640288859606, 'learning_rate': 1.3750000000000002e-06, 'epoch': 49.44}
Step 900/1000 (90.0%) | Loss: 0.0002                                                     90%|█████████ | 900/1000 [1:18:59<06:46,  4.06s/it] 90%|█████████ | 901/1000 [1:19:04<07:35,  4.60s/it] 90%|█████████ | 902/1000 [1:19:09<07:33,  4.63s/it] 90%|█████████ | 903/1000 [1:19:14<07:35,  4.70s/it] 90%|█████████ | 904/1000 [1:19:20<08:13,  5.14s/it] 90%|█████████ | 905/1000 [1:19:25<08:10,  5.16s/it] 91%|█████████ | 906/1000 [1:19:30<07:48,  4.98s/it] 91%|█████████ | 907/1000 [1:19:34<07:31,  4.86s/it] 91%|█████████ | 908/1000 [1:19:39<07:18,  4.77s/it] 91%|█████████ | 909/1000 [1:19:44<07:15,  4.78s/it] 91%|█████████ | 910/1000 [1:19:49<07:10,  4.78s/it]{'loss': 0.0002, 'grad_norm': 0.009420391172170639, 'learning_rate': 1.25e-06, 'epoch': 50.0}
Step 910/1000 (91.0%) | Loss: 0.0002                                                     91%|█████████ | 910/1000 [1:19:49<07:10,  4.78s/it] 91%|█████████ | 911/1000 [1:19:55<07:49,  5.27s/it] 91%|█████████ | 912/1000 [1:20:00<07:31,  5.13s/it] 91%|█████████▏| 913/1000 [1:20:05<07:17,  5.02s/it] 91%|█████████▏| 914/1000 [1:20:09<07:00,  4.89s/it] 92%|█████████▏| 915/1000 [1:20:14<06:53,  4.86s/it] 92%|█████████▏| 916/1000 [1:20:20<07:26,  5.32s/it] 92%|█████████▏| 917/1000 [1:20:25<07:05,  5.13s/it] 92%|█████████▏| 918/1000 [1:20:27<05:43,  4.19s/it] 92%|█████████▏| 919/1000 [1:20:34<06:58,  5.16s/it] 92%|█████████▏| 920/1000 [1:20:39<06:39,  5.00s/it]{'loss': 0.0002, 'grad_norm': 0.007508023176342249, 'learning_rate': 1.125e-06, 'epoch': 50.56}
Step 920/1000 (92.0%) | Loss: 0.0002                                                     92%|█████████▏| 920/1000 [1:20:39<06:39,  5.00s/it] 92%|█████████▏| 921/1000 [1:20:45<06:58,  5.29s/it] 92%|█████████▏| 922/1000 [1:20:51<07:04,  5.45s/it] 92%|█████████▏| 923/1000 [1:20:55<06:36,  5.15s/it] 92%|█████████▏| 924/1000 [1:21:00<06:24,  5.05s/it] 92%|█████████▎| 925/1000 [1:21:07<06:55,  5.55s/it] 93%|█████████▎| 926/1000 [1:21:12<06:35,  5.35s/it] 93%|█████████▎| 927/1000 [1:21:17<06:21,  5.23s/it] 93%|█████████▎| 928/1000 [1:21:21<06:05,  5.08s/it] 93%|█████████▎| 929/1000 [1:21:26<05:54,  5.00s/it] 93%|█████████▎| 930/1000 [1:21:31<05:47,  4.97s/it]{'loss': 0.0002, 'grad_norm': 0.007697887718677521, 'learning_rate': 1.0000000000000002e-06, 'epoch': 51.11}
Step 930/1000 (93.0%) | Loss: 0.0002                                                     93%|█████████▎| 930/1000 [1:21:31<05:47,  4.97s/it] 93%|█████████▎| 931/1000 [1:21:36<05:38,  4.91s/it] 93%|█████████▎| 932/1000 [1:21:40<05:25,  4.79s/it] 93%|█████████▎| 933/1000 [1:21:47<05:58,  5.34s/it] 93%|█████████▎| 934/1000 [1:21:52<05:37,  5.12s/it] 94%|█████████▎| 935/1000 [1:21:56<05:22,  4.96s/it] 94%|█████████▎| 936/1000 [1:21:58<04:19,  4.06s/it] 94%|█████████▎| 937/1000 [1:22:04<04:56,  4.70s/it] 94%|█████████▍| 938/1000 [1:22:09<04:56,  4.78s/it] 94%|█████████▍| 939/1000 [1:22:15<04:59,  4.91s/it] 94%|█████████▍| 940/1000 [1:22:20<05:07,  5.12s/it]{'loss': 0.0002, 'grad_norm': 0.007962709292769432, 'learning_rate': 8.75e-07, 'epoch': 51.67}
Step 940/1000 (94.0%) | Loss: 0.0002                                                     94%|█████████▍| 940/1000 [1:22:20<05:07,  5.12s/it] 94%|█████████▍| 941/1000 [1:22:25<04:50,  4.93s/it] 94%|█████████▍| 942/1000 [1:22:29<04:38,  4.80s/it] 94%|█████████▍| 943/1000 [1:22:34<04:31,  4.76s/it] 94%|█████████▍| 944/1000 [1:22:39<04:33,  4.89s/it] 94%|█████████▍| 945/1000 [1:22:44<04:26,  4.85s/it] 95%|█████████▍| 946/1000 [1:22:49<04:31,  5.03s/it] 95%|█████████▍| 947/1000 [1:22:54<04:19,  4.90s/it] 95%|█████████▍| 948/1000 [1:22:59<04:13,  4.87s/it] 95%|█████████▍| 949/1000 [1:23:03<04:07,  4.86s/it] 95%|█████████▌| 950/1000 [1:23:09<04:10,  5.01s/it]{'loss': 0.0002, 'grad_norm': 0.008072189055383205, 'learning_rate': 7.5e-07, 'epoch': 52.22}
Step 950/1000 (95.0%) | Loss: 0.0002                                                     95%|█████████▌| 950/1000 [1:23:09<04:10,  5.01s/it] 95%|█████████▌| 951/1000 [1:23:13<04:01,  4.92s/it] 95%|█████████▌| 952/1000 [1:23:18<03:52,  4.84s/it] 95%|█████████▌| 953/1000 [1:23:23<03:45,  4.81s/it] 95%|█████████▌| 954/1000 [1:23:25<03:08,  4.10s/it] 96%|█████████▌| 955/1000 [1:23:32<03:36,  4.80s/it] 96%|█████████▌| 956/1000 [1:23:36<03:29,  4.77s/it] 96%|█████████▌| 957/1000 [1:23:42<03:31,  4.91s/it] 96%|█████████▌| 958/1000 [1:23:47<03:25,  4.90s/it] 96%|█████████▌| 959/1000 [1:23:51<03:17,  4.82s/it] 96%|█████████▌| 960/1000 [1:23:56<03:10,  4.76s/it]{'loss': 0.0002, 'grad_norm': 0.008518615737557411, 'learning_rate': 6.25e-07, 'epoch': 52.78}
Step 960/1000 (96.0%) | Loss: 0.0002                                                     96%|█████████▌| 960/1000 [1:23:56<03:10,  4.76s/it] 96%|█████████▌| 961/1000 [1:24:02<03:22,  5.19s/it] 96%|█████████▌| 962/1000 [1:24:06<03:08,  4.97s/it] 96%|█████████▋| 963/1000 [1:24:11<03:02,  4.94s/it] 96%|█████████▋| 964/1000 [1:24:16<02:56,  4.90s/it] 96%|█████████▋| 965/1000 [1:24:21<02:50,  4.87s/it] 97%|█████████▋| 966/1000 [1:24:29<03:16,  5.77s/it] 97%|█████████▋| 967/1000 [1:24:33<02:58,  5.40s/it] 97%|█████████▋| 968/1000 [1:24:38<02:44,  5.15s/it] 97%|█████████▋| 969/1000 [1:24:43<02:35,  5.02s/it] 97%|█████████▋| 970/1000 [1:24:47<02:26,  4.87s/it]{'loss': 0.0002, 'grad_norm': 0.007994560524821281, 'learning_rate': 5.000000000000001e-07, 'epoch': 53.33}
Step 970/1000 (97.0%) | Loss: 0.0002                                                     97%|█████████▋| 970/1000 [1:24:47<02:26,  4.87s/it] 97%|█████████▋| 971/1000 [1:24:54<02:36,  5.38s/it] 97%|█████████▋| 972/1000 [1:24:56<02:02,  4.38s/it] 97%|█████████▋| 973/1000 [1:25:03<02:17,  5.10s/it] 97%|█████████▋| 974/1000 [1:25:07<02:08,  4.93s/it] 98%|█████████▊| 975/1000 [1:25:13<02:07,  5.10s/it] 98%|█████████▊| 976/1000 [1:25:19<02:12,  5.53s/it] 98%|█████████▊| 977/1000 [1:25:25<02:07,  5.53s/it] 98%|█████████▊| 978/1000 [1:25:30<02:01,  5.54s/it] 98%|█████████▊| 979/1000 [1:25:37<02:01,  5.78s/it] 98%|█████████▊| 980/1000 [1:25:41<01:48,  5.42s/it]{'loss': 0.0002, 'grad_norm': 0.008393831551074982, 'learning_rate': 3.75e-07, 'epoch': 53.89}
Step 980/1000 (98.0%) | Loss: 0.0002                                                     98%|█████████▊| 980/1000 [1:25:41<01:48,  5.42s/it] 98%|█████████▊| 981/1000 [1:25:46<01:37,  5.14s/it] 98%|█████████▊| 982/1000 [1:25:50<01:29,  4.97s/it] 98%|█████████▊| 983/1000 [1:25:55<01:22,  4.85s/it] 98%|█████████▊| 984/1000 [1:26:00<01:17,  4.85s/it] 98%|█████████▊| 985/1000 [1:26:04<01:11,  4.75s/it] 99%|█████████▊| 986/1000 [1:26:09<01:06,  4.75s/it] 99%|█████████▊| 987/1000 [1:26:14<01:01,  4.73s/it] 99%|█████████▉| 988/1000 [1:26:18<00:56,  4.72s/it] 99%|█████████▉| 989/1000 [1:26:24<00:53,  4.90s/it] 99%|█████████▉| 990/1000 [1:26:26<00:40,  4.09s/it]{'loss': 0.0002, 'grad_norm': 0.00776115944609046, 'learning_rate': 2.5000000000000004e-07, 'epoch': 54.44}
Step 990/1000 (99.0%) | Loss: 0.0002                                                     99%|█████████▉| 990/1000 [1:26:26<00:40,  4.09s/it] 99%|█████████▉| 991/1000 [1:26:34<00:47,  5.30s/it] 99%|█████████▉| 992/1000 [1:26:40<00:43,  5.38s/it] 99%|█████████▉| 993/1000 [1:26:44<00:35,  5.10s/it] 99%|█████████▉| 994/1000 [1:26:49<00:30,  5.07s/it]100%|█████████▉| 995/1000 [1:26:54<00:24,  4.94s/it]100%|█████████▉| 996/1000 [1:26:58<00:19,  4.85s/it]100%|█████████▉| 997/1000 [1:27:03<00:14,  4.74s/it]100%|█████████▉| 998/1000 [1:27:07<00:09,  4.71s/it]100%|█████████▉| 999/1000 [1:27:12<00:04,  4.71s/it]100%|██████████| 1000/1000 [1:27:17<00:00,  4.69s/it]{'loss': 0.0002, 'grad_norm': 0.010914517566561699, 'learning_rate': 1.2500000000000002e-07, 'epoch': 55.0}
Step 1000/1000 (100.0%) | Loss: 0.0002                                                     100%|██████████| 1000/1000 [1:27:17<00:00,  4.69s/it]{'loss': 0.0002, 'grad_norm': 0.008152014575898647, 'learning_rate': 0.0, 'epoch': 55.56}

  0%|          | 0/49 [00:00<?, ?it/s][A
  4%|▍         | 2/49 [00:01<00:38,  1.21it/s][A
  6%|▌         | 3/49 [00:03<00:48,  1.06s/it][A
  8%|▊         | 4/49 [00:04<00:53,  1.20s/it][A
 10%|█         | 5/49 [00:06<00:59,  1.34s/it][A
 12%|█▏        | 6/49 [00:08<01:14,  1.74s/it][A
 14%|█▍        | 7/49 [00:10<01:11,  1.70s/it][A
 16%|█▋        | 8/49 [00:11<01:06,  1.62s/it][A
 18%|█▊        | 9/49 [00:13<01:06,  1.67s/it][A
 20%|██        | 10/49 [00:15<01:04,  1.66s/it][A
 22%|██▏       | 11/49 [00:16<01:03,  1.66s/it][A
 24%|██▍       | 12/49 [00:18<01:00,  1.64s/it][A
 27%|██▋       | 13/49 [00:20<01:01,  1.70s/it][A
 29%|██▊       | 14/49 [00:21<00:57,  1.66s/it][A
 31%|███       | 15/49 [00:23<00:57,  1.68s/it][A
 33%|███▎      | 16/49 [00:25<00:54,  1.67s/it][A
 35%|███▍      | 17/49 [00:26<00:53,  1.68s/it][A
 37%|███▋      | 18/49 [00:28<00:52,  1.71s/it][A
 39%|███▉      | 19/49 [00:29<00:48,  1.61s/it][A
 41%|████      | 20/49 [00:31<00:48,  1.69s/it][A
 43%|████▎     | 21/49 [00:34<00:53,  1.91s/it][A
 45%|████▍     | 22/49 [00:36<00:53,  1.97s/it][A
 47%|████▋     | 23/49 [00:37<00:48,  1.86s/it][A
 49%|████▉     | 24/49 [00:39<00:43,  1.75s/it][A
 51%|█████     | 25/49 [00:41<00:40,  1.71s/it][A
 53%|█████▎    | 26/49 [00:42<00:37,  1.61s/it][A
 55%|█████▌    | 27/49 [00:43<00:34,  1.56s/it][A
 57%|█████▋    | 28/49 [00:45<00:33,  1.61s/it][A
 59%|█████▉    | 29/49 [00:46<00:30,  1.51s/it][A
 61%|██████    | 30/49 [00:49<00:32,  1.70s/it][A
 63%|██████▎   | 31/49 [00:51<00:33,  1.86s/it][A
 65%|██████▌   | 32/49 [00:52<00:30,  1.78s/it][A
 67%|██████▋   | 33/49 [00:54<00:26,  1.65s/it][A
 69%|██████▉   | 34/49 [00:56<00:25,  1.70s/it][A
 71%|███████▏  | 35/49 [00:58<00:25,  1.82s/it][A
 73%|███████▎  | 36/49 [00:59<00:21,  1.68s/it][A
 76%|███████▌  | 37/49 [01:01<00:19,  1.63s/it][A
 78%|███████▊  | 38/49 [01:02<00:18,  1.64s/it][A
 80%|███████▉  | 39/49 [01:04<00:17,  1.79s/it][A
 82%|████████▏ | 40/49 [01:06<00:15,  1.76s/it][A
 84%|████████▎ | 41/49 [01:08<00:14,  1.81s/it][A
 86%|████████▌ | 42/49 [01:11<00:14,  2.05s/it][A
 88%|████████▊ | 43/49 [01:13<00:12,  2.06s/it][A
 90%|████████▉ | 44/49 [01:15<00:10,  2.13s/it][A
 92%|█████████▏| 45/49 [01:17<00:08,  2.13s/it][A
 94%|█████████▍| 46/49 [01:19<00:05,  1.94s/it][A
 96%|█████████▌| 47/49 [01:20<00:03,  1.88s/it][A
 98%|█████████▊| 48/49 [01:21<00:01,  1.62s/it][A
100%|██████████| 49/49 [01:22<00:00,  1.25s/it][A                                                     
                                               [A100%|██████████| 1000/1000 [1:29:04<00:00,  4.69s/it]
100%|██████████| 49/49 [01:45<00:00,  1.25s/it][A
                                               [ASome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}
There were missing keys in the checkpoint model loaded: ['proj_out.weight'].
                                                     100%|██████████| 1000/1000 [1:29:10<00:00,  4.69s/it]100%|██████████| 1000/1000 [1:29:10<00:00,  5.35s/it]
{'eval_loss': 0.9017106890678406, 'eval_wer': 68.63057324840764, 'eval_runtime': 107.0386, 'eval_samples_per_second': 1.803, 'eval_steps_per_second': 0.458, 'epoch': 55.56}
{'train_runtime': 5350.7108, 'train_samples_per_second': 2.99, 'train_steps_per_second': 0.187, 'train_loss': 0.1012511277346639, 'epoch': 55.56}
